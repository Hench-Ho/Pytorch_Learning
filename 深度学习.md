# 论文阅读

## NT_Xent-AM损失函数的提出（24.6）

**内容**：
类似于监督学习中的AM-Softmax损失函数加边缘（Margin）和AAM-Softmax损失函数加角边缘（Margin）的方式，在自监督学习的对比学习中提出了对NT-Xent损失函数加边缘（Margin）的方法（NT-Xent-AM）并在SimCLR和MoCo框架上做实验。与以往的不同之处，在网络输出没有加入投影层RELU函数，而是使用Self-Attentive Pooling (SAP)提取Representation

**论文标题：** 
*Additive Margin in Contrastive Self-Supervised Frameworks to Learn Discriminative Speaker Representations*

**创新点：**

-  对于自监督学习（SSL）方法SimCLR和MoCo增加边际（Margin）的重要性
- 提出NT_Xent-AM损失对SSL
  *原本损失函数：*

![1750925812417](深度学习.assets/1750925812417.png)

*加入边界Margin后：*

![1750925866972](深度学习.assets/1750925866972.png)

**疑惑：**
 **i-vertor向量**（Front-End Factor Analysis for Speaker Verification）使用在无监督学习中；**d-vertor**（Deep neural networks for small footprint text-dependent speaker verification）和**x-vertor向量**（X-Vectors: Robust DNN Embeddings for Speaker Recognition）使用在深度神经网络。自监督学习对比式训练的方式：**SimCLR**（原始论文：*A Simple Framework for Contrastive Learning of Visual Representations*；语音中应用论文：Contrastive Self-Supervised Learning for Text-Independent Speaker Verification）和**MoCo**（原始论文：*Momentum Contrast for Unsupervised Visual Representation Learning*；语音中应用论文：Self-Supervised Text-Independent Speaker Verification Using Prototypical Momentum Contrastive Learning）

**补充：**
**SimCLR:**（在语音中有个PCL（progressive contrastive learning））
**核心思想：**
通过数据增强将batch_size中N个样本转化成2N个样本。假设有一个样本A，样本A和增强后的样本A’看做positive pairs，样本A与batch_size中的其他样本（2N-2个样本）看做negative pairs，通过计算正样本的损失，反向传播计算参数。

![1750998908737](深度学习.assets/1750998908737.png)

**批量大小：**
需要使用==较大的batch_size==，只有足够多的负样本数，才能是模型学习的更好。较大的batch_size对于GPU的性能条件更为苛刻，所以使用SimCLR框架训练自监督模型需要==使用高性能GPU==。
增强数据：比较==依赖数据增强==

**损失函数：**
NT-Xent Loss

![1751102851823](深度学习.assets/1751102851823.png)

缺点： 比较依赖于batch_size中的负样本数量，负样本数量必须足够大，效果才会好。但是对于一般的GPU而言没有太大的显存，使得该模型的训练条件比较苛刻。

**MoCo：**
**核心思想：**
 以往的SimCLR是比较依赖该批次中**负样本**，但是在MoCo中使用动态字典将负样本的**特征**保存（同时动态更新字典），同时对于样本编码器使用两个不同的Encoder，通过计算正样本的loss反向传播更新encoder编码器，然后将encoder更新后的参数通过动量系数传给momentum encoder编码器，可以使用小样本训练。

![1751000720051](深度学习.assets/1751000720051.png)

**损失函数：**
InfoNCE Loss

![1751102792712](深度学习.assets/1751102792712.png)

**VAD：**
 是一种用于检测音频信号中是否存在人声的技术，主要用于**区分语音段和非语音段**（如静音、背景噪声等）



## 提出数据增强、MoCo队列聚类和ProtoNCE-loss（21.2）

---

**核心思想：**
通过分析SimCLR和MoCo方法，发现==MoCo方法中的Queue队列可以更好的学习到说话人的embedding==，同时发现==WavAug渐进数据增强方法对于说话人识别更有利==，SpecAug方法稍微差一点。==提出了一种聚类的新方法==（在MoCo中Queue队列使用聚类方法，更好的分辨其中的positive pairs 和negative pairs）同时在聚类上==提出了一个损失函数ProtoNCE-Loss==。

![1751122888140](深度学习.assets/1751122888140.png)

**论文标题：**
*Self-supervised Text-independent Speaker Verification using Prototypical Momentum Contrastive Learning*

**创新点：**

- **聚类：**在Moco中Queue队列中使用聚类的方法，能够更好的区别出哪个是positive pairs 和negative pairs，防止将positive pairs 识别成negative pairs。
- **损失函数：**ProtoNCE-loss

![1751122487238](深度学习.assets/1751122487238.png)

C~s~是指正样本的类，C~j~是指负样本的类，V~qi~是指原样本。其中φ等于以下公式：

![1751123470071](深度学习.assets/1751123470071.png)

其中，c是类的中心，V~i~是embedding，Z是embedding总数量，**e是一个稳定的小常数**。

**最终的损失函数：**前面是MoCo的InfoNCE Loss，后面是ProtoNCE-loss。

![1751122748266](深度学习.assets/1751122748266.png)

**补充：**
**WavAug数据增强：**
直接在原始波形（时域信号）上操作，通过模拟真实环境中的音频扰动（如噪声、混响、音量变化等）来增强数据。例如：添加背景噪声、时域掩蔽（屏蔽一段波形）、速度扰动（改变音频播放速度）等。

**SpecAug数据增强：**
在频谱图（时频域，如梅尔频谱）上操作，通过对频谱图的局部区域进行掩蔽或变形来增强数据。例如：频率通道掩蔽（屏蔽某些频段）、时间掩蔽（屏蔽一段时间）、频率扭曲（扭曲频带）等。

## PCL自监督学习方法（22.6）

---

**核心思想：** 
通过**对MoCo和ProtoNCE损失函数进行改进**，通过分析Momentum Encoder后的Memory Queue中的负样本特征的统计特性（Q），==计算其相似矩阵A（affinity matrix）==，然后再进行一个聚类，同时==对ProtoNCE-Loss函数进行改进==。==根据统计特性动态估计Queue队列中每一步的聚类数目==。目的就是想在每个batch_size中**分类出和原说话人相同身份和不同身份的**。

![1751124057563](深度学习.assets/1751124057563.png)

**论文标题：** 
*Progressive contrastive learning for self-supervised text-independent speaker verification*

**创新点:** 
对于**MoCo模型加入聚类方法**，同时**改进自监督学习ProtoNCE模型的ProtoNCE-Loss函数**（以往的ProtoNCE-Loss是对整个数据集进行聚类），改进后的ProtoNCE-Loss是通过对每一个batch_size中的数据进行动态聚类。使网络能够提取出更具有判断性的representation，提升鲁棒性。

**疑惑：** 
在Memory  Queue中是如何使用聚类方法的。

**补充：** 
**对ProtoNCE-loss改进的PCL损失函数：**

![1751124260274](深度学习.assets/1751124260274.png)

V~i~ ，C~j~ ，C~i~和ProtoNCE-loss一致，但是φ略有不同：

![1751124722787](深度学习.assets/1751124722787.png)

其中的β是一个常数10.

**最终的损失函数：**

![1751124995860](深度学习.assets/1751124995860.png)

## 可迭代的自监督学习（LGL）（22.7）

---

**核心思想：**
分为两个阶段。==StageI==：通过自监督的对比学习训练出可以高效提取特征的网络结构。==StageII==：使用训练好的网络结构（Encoder）提取Speaker representation（提取出一个batch_size中的representation）==对提取出的特征进行聚类生成伪代码（不一定正确的伪代码）==。同时==使用相同的网络结构（Encoder）对经过语音增强后的同一个batch_size中的样本提取Speaker representation==，并==使用生成伪标签和真实标签计算Loss==。当伪标签正确时这里的Loss会减低的很快，相反伪标签错误时Loss减低速率很慢。==LGL框架可以剔除伪标签错误的网络模型参数，保留伪标签正确的网络模型参数==，同时将参数更新到Encoder编码器。通过不断地迭代。。。。。

![1751209844323](深度学习.assets/1751209844323.png)

**论文标题：**
*Self-supervised Speaker Recognition with Loss-gated Learning*

**创新点：**

- 通过预训练的模型提取Speaker representation进行聚类生成伪标签，通过对比伪标签和真实标签剔除错误伪标签，使用正确的伪标签更新的网络参数再次赋给Encoder不断迭代。。。。
- 通过分析错误伪标签和正确伪标签进行分类时Loss降低的速率，进而判断哪个为正确的伪标签。

![1751209268731](深度学习.assets/1751209268731.png)

- LGL使用：在经过多轮的训练，提取Loss最小的轮次，将更新的网络参数赋给Encoder不断迭代。。。。

**疑问：**
这里是如何将伪标签和真实标签做Loss的？？？还有每个Encoder里提取的特征是不是一个batch_size？？？

![1751209405897](深度学习.assets/1751209405897.png)

## SSL模型学习中的偏见（24.9）

---

**核心思想：**
发现最新SSL模型比如：Wav2Vec 2.0、HuBERT和MelHuBERT对于数据的训练都有偏见。（识别声音时会自动的将悲伤的声音识别为年龄大的，将愤怒的声音识别为年轻人）本文提出==通过压缩的方法将base模型slim、small变化==（slim和small二者模型参数相差不大）降低模型对数据的偏见。压缩方法有剪枝（Pruning包括Head Pruning、Row Pruning、Weight Pruning）、蒸馏（Distillation）。

![1751257188905](深度学习.assets/1751257188905.png)

**论文标题：**
*On the social bias of speech self-supervised models*

**创新点：**

- 模型剪枝：
  - Head 裁剪：如下图（a）（b）模型对性别数据偏见有所缓解，但是对国籍和年龄数据偏见没有变化。
  - Row 裁剪：如下图（c）（d）模型对性别、国籍、年龄数据偏见均有所下降。
  - Weight 裁剪：如下图（e）（f）随着裁剪程度加大，国籍和年龄偏见有所减低，但性别偏见反而上升。

![1751257532418](深度学习.assets/1751257532418.png)

- 模型蒸馏：

![1751257568298](深度学习.assets/1751257568298.png)

**补充：**
**模型蒸馏：**
将大型复杂模型（教师模型）的知识迁移到小型轻量模型（学生模型）中，使小模型能模仿大模型的行为。

**模型剪枝：**
通过移除神经网络中的冗余部分（如权重、神经元、通道等），减少模型大小和计算量。

- **结构化剪枝**：移除整个通道、层或块（适合硬件加速）。
- **非结构化剪枝**：移除单个不重要的权重（需稀疏计算支持）。
- **迭代剪枝**：逐步剪枝并微调，避免性能骤降。
- **重要性准则**：根据权重绝对值、梯度、激活值等判断重要性。

## SSL自监督学习是否可以移除下游模型？（24.9）

---

**核心思想：**
首先验证了SSL模型提取了说话人的哪些信息、SSL同一模型不同参数对说话人特征提取的效果、SSL模型是否依靠滤波器组提取说话人特征和去除下游模型（ECAPA-TDNN）中的帧级编码器（多个卷积层提取每一帧特征）。（表明下游模型不需要使用帧级编码器（和滤波器组相同的帧处理方式）只使用注意力池化机制就可以实现SSL提取特征的效果）

![1752566973681](深度学习.assets/1752566973681.png)

**论文标题：**
*Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?*

**创新点：**

- 解释了说话人信息是如何被SSL模型捕获的
- 解释了SSL模型提取特征不是依赖于滤波器组
- 解释了较大的SSL模型对说话人特征提取效果更好
- 简化下游模型参数的同时提高了对说话人识别的效果

## 自监督模型自动退出训练并适应下游模型（24.9）

---

**核心思想：**
在自监督模型的隐藏层加入分支函数，通过计算k层的分支损失loss，测量每层分类器输出的交叉熵对模型进行停止。在训练下游模型时，计算数据集的复杂度的交叉熵E与阈值τ进行比较，当E＜τ时，f函数=1此时停止训练，反之继续训练。将自监督模型和下游模型结合在一起，通过该论文提出的新方法通过对数据集的自适应，自动退出自监督模型和下游模型的训练，节省不必要的训练时间。

![1752566896340](深度学习.assets/1752566896340.png)

**论文标题：**
*DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation Models*

**创新点：：**

- 根据自监督loss判断模型何时退出训练
- 通过计算训练集的复杂度，自适应的退出训练，节省没必要的训练。

## 语音鉴伪中的语音增强技术用在PA场景（重放伪造）（24.11）

---

**核心思想：**
使用ASVspoof2019PA训练集和验证子集进行训练，使用然后使用ASVspoof2019和ASVspoof2021的Real数据集进行验证；然后对数据集进行数据增强（DA）有：时域和频域掩码、增加噪声（增加分贝（高斯白噪声））、调整房间脉冲（不同大小房间的数据收集RIRS）、数据混合（在训练数据集中加入新的合成特征混合）；使用LCNN模型进行训练。

**论文标题：**
*Data augmentation techniques for Physical Access in voice anti-spoofing*

**创新点：**

- 针对PA场景（重放、录制、电话等）的数据进行多种数据增强方式得到数据然后训练。
- 几种在PA场景下的数据增强方式

## 自监督embedding提取和合成语音增强对语音鉴伪鲁棒性的影响（24.9）

---

**核心思想：**
使用Wav2Vec2自监督模型预训练（冻结不做任何操作），下游模型使用Adapter、Frame-wise processing、Time pooling和Scoring layer，使用Adapter对SSL中的Transformer每一层（总共24层）提取特征进行层归一化最后融合为一个特征；Frame-wise processing层使用RELU函数和Dropout对embedding进行降维；Time pooling使用统计池化实现正则化；最后计算余弦相似度求得分。数据集采用原ASVspoof 2019数据集和通过4个不同声码器对ASVspoof 2019微调得到数据。

![1752566723610](深度学习.assets/1752566723610.png)

**论文标题：**
*Exploring Self-supervised Embeddings and Synthetic Data Augmentation for Robust Audio Deepfake Detection*

**创新点：**

- SSL隐含层足以包含语音的所有特征
- 用声码器技术从训练数据中的真实语音中创建新的假样本（相当于使用语音合成技术生成虚假的语音）

## 语音鉴伪中的数据增强新方法RawBoost

---

**核心思想：**
在语音鉴伪LA数据集上使用三种方法线性和非线性卷积噪声、脉冲信号依赖的加性噪声和平稳信号无关的加性噪声对数据集进行处理（不添加任何外部数据集的情况下，Raw Boost对编码、传输、麦克风和放大器、线性和非线性失真等引起的干扰变化进行建模），在同一个模型上结果比使用WavAugment（加噪声、背景声、语速加快等）和SpecAugment（频率掩码、时间掩码等）数据增强方法表现好。

![1752566381321](深度学习.assets/1752566381321.png)

**论文标题：**
*RawBoost: A Raw Data Boosting and Augmentation Method applied to Automatic Speaker Verification Anti-Spoofing*

**代码：**
https://github.com/asvspoof-challenge/2021/tree/main/LA/Baseline-RawNet2

**创新点：**

- 不需要额外的数据库（比如RIRS、MUSAN等）
- Raw Boost对编码、传输、麦克风和放大器、线性和非线性失真等引起的干扰变化进行建模
- 效果比WavAug和SpecAug好

**想法：**
==可以将该数据增强的方法用在自监督学习上，增强和增大数据集==

## 对数据池中的数据进行自主学习（22.10）

---

**核心思想：**
首先选择种子训练数据集U~seed~（此数据集中首先选择的是专业领域较强的数据集（鉴伪方面的）），通过主动选择有用数据和主动移除无用数据的方法每次抽取U~pool~（更为混杂的数据集，其中有有效数据和无效数据）中L个数据放入U~seed~训练集中。判断有用数据和无用数据的方法有三种：基于负能量的确定性评分、基于对抗样本的距离、随机得分。（该方法就是能自动的从大规模数据集上挑选有用、有效的数据加入到训练数据集上（*C**M：伪造对抗***））

![1752669331464](深度学习.assets/1752669331464.png)

**论文标题：**
*Investigating Active-learning-based Training Data Selection for Speech Spoofing Countermeasure*

**代码：**
https://github.com/niiyamagishilab/project-NN-Pytorch-scripts

**创新点：**

- 自主选择大规模数据中的有效数据（充分利用有效的数据集）
- 使用较多的数据集训练模型可以提高模型的泛化性

**想法：**
==在预训练的自监督模型上加入下游模型，在大规模的U~pool~数据集上使用该论文的方法微调，能够快速的训练并适用特定场景。==

## ASVspoof5竞赛使用SSL预训练（24.8）

---

**核心思想：**
使用数据扩展（在训练集或者验证集中加入其他数据集）和数据增强（在语音`高频段会出现高频间隙`，严重影响CM的性能。使用`低通滤波`将高频滤除，但在低频仍含有频率间隙，所以采用`频率掩码`方式数据增强；加入RIR和MUSAN数据集）在预训练模型SSL（WavLM、Wav2vec2-large、UniSpeech预训练模型）上提取隐含层或者最后一层特征输入下游网络。

![1752829169800](深度学习.assets/1752829169800.png)

**论文标题：**
*Temporal variability and multi-viewed self-supervised representations to tackle the ASVspoof5 Deepfake Challenge*

**创新点：**

- 提取SSL模型中的隐含层和最后一层作比较，发现`隐含层的特征提取`效果对于下游模型效果更好。
- 首先`查看数据集`虚假和真实语音数量、音频时长等信息，找出音频时长平均值。（选择4/6/8/10/12/14/16秒分别做实验）
- 首先使用一个模型wavlm-base-5提取特征，使用不同的数据集训练，查找哪个数据集效果更好。
- 选出最好的数据集`5trndev`，使用不同的SSL模型（包括单个模型的不同层）训练数据，查找到`unispeech-base-5`（UniSpeech第五层提取的特征）效果最好。
- 选择5trndev数据和unispeech-base-5模型，再对数据集`进行数据增强`，最后发现（RIR + MUSAN + Freqmask(0.3)）效果最好。
- 选择5trndev数据（使用RIR + MUSAN + Freqmask(0.3）数据增强方法）和unispeech-base-5模型，再对数据集进行扩展，加入19LA、Codecfake、MLAAD等数据集，发现效果没有提升，故舍去。
- 使用单一模型对不同音频长度训练，同时使用不同模型对同一时间长度音频训练。
- 将单一模型的不同层和多个模型的特定层提出的特征进行融合，同时使用单一模型的特定层处理不同的时间长度音频，进行融合最后得到最优结果。

**想法：**
==可以将实验的整个训练思路运用到自己的训练中==

## 对于语音鉴伪伪造源属性分类（22.12）

---

**核心思想：**
对于伪造语音属性分类，提高系统的鲁棒性和语音鉴伪的准确率。对于逻辑攻击伪造（TTS、VC）的算法进行分类，主要目的不是对于伪造算法的分类，而是提高鉴别伪造语音的准确性。利用`多任务学习（多任务训练）`的方法对不同伪造属性分类，标签分为三类Conversion、Speaker Representation、Waveform Generator，使用不同的后端分类器共用前向的模型得到3中loss，最后将3种loss加权和送入Softmax对语音进行鉴伪。

![1753339660900](深度学习.assets/1753339660900.png)

数据集中的分类：

![1753340180933](深度学习.assets/1753340180933.png)

标签分类：

![1753340133794](深度学习.assets/1753340133794.png)

**论文标题：**
*Source Tracing: Detecting Voice Spoofing*
相关论文：*Source Tracing of Audio Deepfake Systems*（供参考，论文写的乱）

**创新点：**

- 提出了一种新方法，通过对`伪造方法属性分类`，然后利用分类后的欺骗概率进一步预测语音真伪。
- 将语音算法分为三类：Conversion、Speaker Representation、Waveform Generator（也就是三个标签），分类标准值得学习。
- 多任务学习的方法再加上融合方法，`即可分类伪造属性，又可以预测语音真伪`。

**缺点：**

- 对于算法分类太少，可以结合最新的TTS、VC算法
- Speaker Representation标签预测准确度太低

**想法：**
==使用11论文中的池化思想将目前最新的TTS、VC算法放入数据池中进行学习，提升泛化性和鲁棒性==

## 使用多种数据增强方法ASVspoof5竞赛（24.8）

---

**核心思想：**
对原始语音和Mel谱做数据增强，对于原始语音使用Waveform augmentations：Time masking（CutOut）、Background noise（MUSAN加入SNR）、RawBoost、Speed perturbation、Codec（MP3, G.722, OGG, AAC,  OPUS）、Audio shuffling；对于Mel谱使用CutMix方法（SpecAugment没有效果）；使用时间切片（切成固定长度的语音，比如4s）；使用声码器生成新的伪造语音。CM系统使用ResNet34模型对于不同的数据增强方式进行训练，ASV系统使用ResNet-221和ResNet-293的预训练模型（对Voxceleb2数据集）。无论是CM还是ASV最终都是使用`模型集成`的方法对最优的几个模型进行加权得到最终模型。

![1753607738780](深度学习.assets/1753607738780.png)

**论文标题：**
*ParallelChain Lab's anti-spoofing systems for ASVspoof 5*

**创新点：**

- 使用模型集成（对最优的几个模型进行加权得到最终模型对测试集进行预测）的方法。
  - 加权平均集成（手动权重）
  - 可学习权重的集成（元模型）
- 对于训练集中真实和伪造语音样本比例进行调整（两者比例不能过大），由于真实样本少，可增加真实样本的重复频率实现比例平衡。
- 加入声码器对ASVspoof5数据集中的真实样本生成虚假样本，让模型学习区分声码器的能力。

**想法：**
==模型集成方法值得学习、训练集真假样本比例不能差距太大==

## 注意力融合SSL隐藏层的各层embedding

---

**核心思想：**
使用预训练的SSL模型提取各层的embedding，将特征拼接起来（L, T, H）得到（d），通过时间特征融合（L, 1, H）（对于T维进行求均值torch.mean）得到（e），使用带有权重的全连接层对H维进行压缩得到（L, 1, 1）(f)，使用torch.squeeze()方法将（f）中的1维删除得到注意力权重（g），使用得到的注意力权重去压缩原特征（d）得到（h）,最后通过多头注意力机制得到最终的特征向量（j）。使用多头注意力机制分析SSL模型每一层的重要性，最终得到在中间层提取出来的特征效果最好。

![1754639423160](深度学习.assets/1754639423160.png)

**论文标题：**
*Attentive Merging of Hidden Embeddings from Pre-trained Speech Model for Anti-spoofing Detection*

**创新点：**

- 引入多头注意力，将SSL提取出来的特征进行融合
- 引入本文提出的AttM和以往的LinM融合特征向量
- 使用LinM分析SSL中24个变压器编码器的隐藏嵌入进行归一化线性加权，得到每层对训练的影响程度

![1754641530668](深度学习.assets/1754641530668.png)

**想法：**
==如果使用双阶段训练CM，在第一阶段引用多头注意力机制对虚假语音的算法类别进行分类，分析不同合成算法的权重对结果的影响，从而对于不同的数据集赋予不同的权重==

**疑问：**

- ==权重影响热力图是如何画的？==

## Matcha-TTS（NAR）（24.1）

---

**论文标题：**

*Matcha-TTS: A fast TTS architecture with conditional flow matching*

**核心思想：**

使用1D-CNN（降低显存）、旋转编码器（RoPE）（降低显存、对长序列的泛化性更好）、Transformer（解码器）和最新的生成方法OT-CFM（训练更快、使用更少的步数就能达到不错的效果）。通过预测时长模型和对齐模型预测出均值Mel谱作为解码器的输入（`而不是直接将噪声输入`）。（参考Grad-TTS、Glow-TTS和Voicebox）

采用最优传输条件流匹配（OT-CFM）公式如下：其中σ~min~取一个很小的值，X~1~是目标，X~0~是初始（先验分布），使用该公式定义flow，使得初始的噪声一步步生成目标。

![1758262067518](深度学习.assets/1758262067518.png)

![1757939876572](深度学习.assets/1757939876572.png)

**创新点：**

- 解码器使用1D-CNN降低显存
- 在编码器中使用Transformer结构并使用旋转编码器（RoPE）首次在SDE、ODE中使用RoPE使用，可降低显存
- 不需要外部对齐，让模型自己学习文本和语音的对齐关系（与Matcha模型相反，Voicebox需要外部对齐）
- 使用OT-CFM生成语音，速度更快、效果更好。

**补充：**

- 使用代码进行训练时，需要使用[tacotron2/filelists at fc0cf6a89a47166350b65daa1beaa06979e4cddf · NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2/tree/fc0cf6a89a47166350b65daa1beaa06979e4cddf/filelists)filelist，同时还需要运行Setup中的第5步将DUMMY换成我们数据集的路径。
-  **外部对齐** 是指它不自己学习文本和语音之间的时间对齐关系，而是借助已有的外部模型（比如 ASR 强制对齐工具）来获得 **文本 token 与声学特征的对应信息**，再基于此进行语音生成建模。

## VoiceFlow（NAR）（24.9）

---

**论文标题：**

*VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching*

**核心思想：**

在Flow-matching的基础上加入矫正环节，组成名为`rectified Flow matching`。主要内容就是使用Flow-matching训练后的向量场v再次输入到矫正网络进行校正，`目的是使flow的Path变直`，这样在inference环节就可以使用更少的step预测`（减少推理时间）`。模型如下，首先将文本转化成音素，然后使用Text Encoder对音素进行编码，同时使用Duration Predictor预测每个因素的持续时间，最后使用Duration Adaptor将音素编码和持续时间进行规整，得到的该文本的语义特征。使用条件估计tx~1~ + (1 − t)x~0~生成的t时刻的Mel谱X~t~，将y、X~t~和t作为Vector Field Estimator的输入，预测出最终的向量场（该向量场的方向是直线）

![1758268208769](深度学习.assets/1758268208769.png)

**创新点：**

- 考虑到FLow-matching在单样本path是直线，而整个分布预测时呈现出曲线的形式。引入rectified flow将分布预测的path拉直，减少了推理时间。

  ![1758269235253](深度学习.assets/1758269235253.png)

**补充：**

- 为什么Flow-matching的flow path不是直线？

  因为对于单样本的预测是直线路径，但是在分布中（多样本）就会由于某些复杂的条件变成曲线。比如，只随机生成一个高斯噪声样本点同时只预测一个样本点目标，使用flow-matching过程是一条直线；但是在训练时往往是预测一批数据，那么也会生成一批的高斯噪声，这批噪声在X~t~时刻对应的X~0~和X~1~可能有多对(x_t = *t* * *x_1* + (1 - *t*) * *x_0*)，比如同时走到图1中的交点位置，他到目标1和目标2的位移都是相等的，所以此处的向量场v方向就是水平向右，导致轨迹拐弯。

  ![1758273078748](深度学习.assets/1758273078748.png)

  ![1758269457961](深度学习.assets/1758269457961.png)

## Zipvoice（NAR）（25.8）

**论文标题：**

*ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching*

**核心思想：**

使用Zipformer模型为基础的text Encoder、Flow-matching。没有直接将text token直接得到text Embedding，而是中间加入了`text Encoder（Zipformer）`分析text序列中的上下文关系（有显微的提升）；`Average Upsample`（作用就是将text 和Speech对齐）这里采用的是直接将Speech feature frame length/text token length得到Duration（放大系数），将每个text token重复Duration次得到Text Condition；Speech Condition是通过标签Speech随机掩码70%-100%得到的，向量场模型的输入是：`Text Condition + Speech Condition + 高斯噪声` 三者的Concat；对于Vector Field Estimator（采用Zipformer为骨干网络,使用Flow-matching的方法）

- 训练：

  - 使用`char方法`，将text转化成text token(y)，N是text token的长度，y~i~是第i个text token
  - 上一步的输出作为Embedding和Text Encoder（Zipvoice）的输入，输出`Text Embed`，F是特征维度，N是Text token的长度
  - 上一步的输出y‘作为Average Upsample的输入，`结合Fbank的feature_len`，将Duration扩充并`输出Text Condition`，F是特征维度，T是Mel谱的特征长度（与Speech Condition对齐）
  - 将`Text Condition、Speech Condition和Noisy Speech`进行Concat操作输入到Vector Field Estimator，预测当前位置的向量场与目标的向量场之间的 loss，进行反向传播更新参数。

- 推理：

  - 需要Prompt Text（论文中的y^prompt^）、Target Text（论文中的y^synthesis^）、Prompt Speech（论文中的T^prompt^）和Padding Speech（图中的Zero Padding，论文中的T^synthesis^） 。最终的 **Speech Condition长度是T^synthesis^ + T^prompt^** 

    ![1760270717344](深度学习.assets/1760270717344.png)

  - 使用模型预测向量场，然后使用ODE Solver推理

![1760101399500](深度学习.assets/1760101399500.png)

**补充：**

1. `number of function evaluation（NFE）是什么？`

   - NFE 是指在生成一段语音的过程中，模型需要进行多少次函数计算（forward pass）才能完成采样。更具体地说，对于一个扩散类 TTS 模型，在从噪声反推回干净语音的过程中，需要多步迭代

   - NFE 越大 → 音质好但慢；NFE 越小 → 速度快但质量可能下降。

   - 减少NFE的方法：

     | 方法                          | 代表性模型/思路                              | 核心原理                                              |
     | ----------------------------- | -------------------------------------------- | ----------------------------------------------------- |
     | **加速采样（Fast Sampling）** | DDIM、DPM-Solver                             | 使用确定性采样或高阶近似，跳过部分扩散步              |
     | **知识蒸馏（Distillation）**  | Consistency Models、Progressive Distillation | 把多步采样压缩为少步甚至一步采样                      |
     | **少步扩散 TTS**              | 如 E2 TTS、FastDiff                          | 设计训练策略，让模型在很少的 NFE 下也能生成高质量音频 |
     | **并行采样或缓存**            | 多段语音并行生成                             | 提高实际吞吐率                                        |

2. `Classifier-free guidance (CFG)是什么？`

   - 核心目的是在不依赖额外分类器的情况下，提高生成样本与**条件信息（如文本、音素）的一致性**。
   - Classifier-Free Guidance 是一种在扩散采样过程中增强条件控制的技术，通过混合有条件和无条件预测，使生成的语音更准确、更自然，同时不需要训练额外分类器。
   - 关键 trick 是在**训练时随机丢弃部分条件信息**。让模型既学到有条件生成又学到无条件生成

3. `为什么对Speech进行掩码操作可以使模型具有Zero-shot能力？`

   - 通过在训练中随机掩码语音，让模型学会“补全缺失信息”，也就让它具备了在新说话人、新内容场景下的泛化（Zero-shot）能力。

   - 模型在推理时，面对训练中**未见过的说话人 / 内容 / 语言 / 声音风格**，也能较好地完成任务（如语音合成或转换），不需要专门微调。

4. `在flow-matching预测向量场时，为什么用Zipformer而不用Transformer？`

   - 非常适合向量场（vector field）的预测，使用U-Net网络，可以处理不同维度的feature representations
   - 使用卷积神经网络添补Transformer在短序列上的空缺
   - 复用注意力权重来提升参数效率，提升计算效率、减少参数量

5. `为什么不是直接将文本转化成Text Embedding，而是中间再加入Text Encoder？`

   - 之所以不直接用 Text Embedding，是因为它不能提供丰富的上下文和语义信息。加入 Text Encoder 能让文本表示更适合与语音对齐，从而显著提升合成质量与泛化能力

   - 举个例子

     | 文本            | Token             | Embedding 输出         |
     | --------------- | ----------------- | ---------------------- |
     | “record a song” | [record, a, song] | E1, E2, E3（固定向量） |

     👉 这里 “record” 是名词（唱片）还是动词（录制）embedding 并不知道，因为它不看上下文。

6. 

## 提升基于流匹配的零样本文本转语音（TTS）在含噪音频提示下的生成质量（24.9）

---

**论文标题：**

*An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS*

**核心思想：**

通过数据过滤、掩码语音去噪预训练与随机噪声混合微调，增强模型对噪声的鲁棒性。

- 对预训练数据进行筛选、过滤：将含有噪声和不清晰的语音进行滤除，保存下来高质量的音频。
- 使用类似于WavLM（从噪声输入提取出被掩码部分的清晰音频）模型的去噪策略。
- 一般情况下，都使用清晰音频作为数据集进行微调，但在该paper下，使用的是含噪声概率为P的数据集下进行微调。

## 提升在含噪声Prompt Speech的语音合成质量（24.1）

**论文标题：**

*Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters*

**核心思想：**

在自监督语音表示模型（WavLM）中插入参数高效的适配器，并与TTS模型联合微调，以提取噪声鲁棒的说话人嵌入。

- 使用不同层的SSL对原始Speech进行特征提取，得到两个不同阶段的embedding（Duration和Spec），将这两个embedding加入到不同的时期以提高合成音频的质量。
- 在预训练SSL模型（WavLM）中插入BN（Transformer Adapters）与CNN（CNN Adapters）适配器，仅微调适配器与嵌入模块，使用噪声语音联合训练TTS模型（FastSpeech2）以提取鲁棒说话人嵌入。
- 相较于全参数微调或仅用语音增强，适配器在保持预训练知识的同时提升噪声下的合成质量，且参数量仅为全微调的17% 

## 基于LLM的零样本TTS在噪声音频提示下合成语音质量（25.8）

---

**论文标题：**

*Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising*

**核心思想：**

提出一种在离散Acoustic token域操作的编解码器去噪器，仅预测前两组关键token以降低复杂度并保留说话人信息。

- 在LLM模型中加入Codec denoiser，处理输入进来的Prompt Speech（得到去噪后的Acoustic token），再输入进LLM（和原始一样推理即可）

# 语音合成

---

## TTS的主要任务

---



| 控制维度           | 控制对象                           | 举例                   |
| ------------------ | ---------------------------------- | ---------------------- |
| **音色 (Timbre)**  | 说话人的声音特质                   | 男声、女声、小孩、老人 |
| **韵律 (Prosody)** | 超音段特征（节奏、停顿、音高走势） | 快慢、重音、升调/降调  |
| **情感 (Emotion)** | 短时情绪色彩                       | 开心、悲伤、愤怒       |
| **风格 (Style)**   | 整体说话方式/身份场景              | 播音、讲课、聊天、配音 |

## TTS流程

---

### 语音（speech）处理流程：

---

#### **完整流程图示：**

```text
原始语音波形
       ↓
[神经网络编码器]
       ↓
连续声学特征向量 (类型A) 
       ↓
[Codebook量化] → 生成Speech Token ID
       ↓
Speech Token序列 (整数ID)
       ↓  
[嵌入层查找]
       ↓
Token嵌入向量 (类型B)  # 这是给模型用的向量
       ↓
[语言模型处理]
```

#### 步骤1: 语音波形 → 连续特征向量

```python
# 输入：原始语音波形（连续信号）
audio_waveform = [0.23, -0.45, 0.67, ...]  # 采样点序列

# 通过编码器提取特征（连续向量序列）
encoder = NeuralAudioEncoder()  # 神经网络编码器
continuous_vectors = encoder(audio_waveform)

# 输出：连续的特征向量序列
# 例如：[[0.15, 0.25, 0.35], [0.18, 0.22, 0.41], ...]
```

#### 步骤2: Codebook量化 → Speech Token

```python
# 使用Codebook进行向量量化
codebook = {
    0: [0.10, 0.20, 0.30],  # 码本条目0
    1: [0.50, 0.60, 0.70],  # 码本条目1  
    2: [0.90, 0.05, 0.15],  # 码本条目2
    # ... 共1024个条目
}

speech_tokens = []  # 存储生成的Token ID
quantized_vectors = []  # 存储量化后的向量

for vector in continuous_vectors:
    # 在码本中找最接近的向量
    closest_id = find_closest_codebook_entry(vector, codebook)
    speech_tokens.append(closest_id)  # 例如: 0, 2, 1, 0, ...
    
    # 使用码本中的向量作为近似
    quantized_vec = codebook[closest_id]
    quantized_vectors.append(quantized_vec)

# 现在有了离散的Speech Token序列
```

#### 步骤3: Speech Token → 嵌入向量（用于后续处理）

```python
# 为了让神经网络能处理这些Token，需要再次向量化
token_embedding_matrix = nn.Embedding(num_embeddings=1024, embedding_dim=512)

# 将Token ID转换回向量（但与步骤2的量化向量不同！）
token_vectors = token_embedding_matrix(speech_tokens)

# 这些向量将输入给语言模型进行生成或理解
```

#### 为什么需要这个"绕圈子"的过程？

> **阶段1的目标：信息压缩和离散化**
>
> ```python
> # 从高维连续空间到低维离散空间
> 原始音频: 每秒24000采样点 × 16位 = 384,000比特/秒
> 特征向量: 每秒100帧 × 64维浮点 = 20,480比特/秒  
> Speech Token: 每秒100帧 × 10位(1024个Token) = 1,000比特/秒
> 
> # 压缩比达到384:1！
> ```
>
> **阶段2的目标：适配语言模型**

### 文本（text）处理流程：

---

#### 完整流程图示

```python
原始文本 → [分词器] → Text Token IDs → [嵌入层] → 文本向量
```

#### 步骤1: 文本 → Text Token（直接离散化）

```python
# 使用分词器直接转换
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "Hello world"

# 直接得到离散的Text Token IDs
text_tokens = tokenizer.encode(text)  # [101, 7592, 2088, 102]
# 101: [CLS], 7592: "hello", 2088: "world", 102: [SEP]
```

#### 步骤2: Text Token → 嵌入向量（直接查找）

```python
# 直接使用嵌入层查找
text_embeddings = text_embedding_layer(text_tokens)  # [seq_len, hidden_dim]
```

#### Phoneme Tokens和characters Tokens区别

##### 1.含义

| 名称                 | 含义                                               | 举例（以中文“你好”为例）                         |
| -------------------- | -------------------------------------------------- | ------------------------------------------------ |
| **Character Tokens** | 直接用字符（或文字）作为模型的输入单元             | “你”“好” 或 “n”“i” （拼音字母）                  |
| **Phoneme Tokens**   | 将文字转换成**音素**（发音的最小单位）后再作为输入 | /n i˧˥/ /x aʊ˨˩˦/（或使用拼音音素序列 ni3 hao3） |

##### 2.区别

- **Character Tokens**
  - 保留了原始文字的语义信息。
  - 模型需要**自己学会从文字到语音的发音规则**。
  - 如果发音规律不规则（比如英语），难度大。
- **Phoneme Tokens**
  - 去掉了语义层，直接告诉模型怎么读。
  - 发音信息显式，无需模型学习发音规律。
  - 适合 TTS 场景，发音一致性高。

##### 3.不同语言的适用性

| 语言类型     | Character Tokens         | Phoneme Tokens                 | 原因                   |
| ------------ | ------------------------ | ------------------------------ | ---------------------- |
| 中文         | ✅ 常用（字符本身信息强） | ✅ 常用（尤其是多音字时更准确） | 多音字和声调信息是关键 |
| 英语         | ❌ 发音不规则，难学       | ✅ 发音规则通过字典得到，更可靠 | “read” 就有多种发音    |
| 日语/韩语    | ✅（发音规律性强）        | ✅                              | 可以两者都用           |
| 多语言数据集 | ❌ 可能导致歧义           | ✅ 统一发音层有利于共享         | 跨语言的拼写差异大     |

##### 4.对模型学习难度的影响

Character Tokens：

- 模型必须额外**学习 Grapheme-to-Phoneme（G2P）** 转换。
- 数据量不够时容易出现发音错误。
- 优点是能保留语义，对有些 NLP-TTS 融合任务更友好。

Phoneme Tokens：

- 降低学习难度，模型直接学习发音到声学特征的映射。
- 在小数据集或多语言场景下更稳定。
- 但如果字典不全，预处理复杂度高。

`为什么character token必须额外学习G2P？`

- Character Tokens 不包含发音信息，只是拼写。
- 不同 grapheme 到 phoneme 的映射可能一对多或多对一。
- 模型必须“额外学习”这套 G2P 转换关系，才能从文字生成正确的语音。
- 使用 Phoneme Tokens 相当于**提前替模型完成了 G2P 转换**，大幅降低了训练难度和错误率。

##### 5.对于不同数据集的影响

📌 1. **小规模单语言数据集**

- **Phoneme Tokens** 往往效果更好，收敛更快，语音更自然。
- Character Tokens 因数据不足，难以学好发音规律，容易出错。

📌 2. **大规模单语言数据集**

- Character Tokens 也能达到很好的效果。
- Phoneme Tokens 稳定性更强，尤其在发音不规则的语言上。

📌 3. **多语言或代码混合数据集（Code-Switching）**

- Phoneme Tokens 有天然优势：
  - 不同语言的发音空间可以统一处理。
  - 避免因为拼写差异导致的 token 冲突。
- Character Tokens 需要复杂的字典映射或分词策略。

## TTS模型架构

---

**“自回归 vs 非自回归”主要指的是解码声学特征或波形时的生成方式**。

**自回归缺点：**在重复词语上容易误识别，多次重复一个词语；鲁棒性差；时间可控性不好。

**非自回归缺点：**在实时交互的场景（包含空窗期）；对其预测比较困难（text和speech的对齐）；相较于自回归来自然度不是特别好。

- **非自回归（non-autoregressive ,NAR）：**给定`input`输入`并行`的生成整个`speech sequence`

  1. **Transformer-based Methods：**FastSpeech、FastSpeech 2、FastPitch
  2. **VAE-based Methods：**Parallel Tacotron、
  3. **Diffusion-based Methods：**NaturalSpeech 2、NaturalSpeech 3、DEX-TTS、E3 TTS、
  4. **Flow-based Methods：**Audiobox、P-Flow、VoiceBox、FlashSpeech、E2 TTS、F5-TTS、E1 TTS

- **自回归（autoregressive ,AR）：**`当前输出的y `都要依靠`之前所有输出`和输入`input`

  1. **RNN-based Methods：**Prosody-Tacotron、Global Style Tokens、MsEmoTTS、
  2. **LLM-based Methods：**VALL-E、SpearTTS、Make-a-Voice、FireRedTTS、CoFi-Speech、Cosyvoice

- **混合式（既有自回归又有非自回归）：**

  **上层 (token 生成)**：多数采用 **自回归 (AR)**，保证自然度与时序建模

  **下层 (解码 Mel / 波形)**：多数采用 **非自回归 (NAR)**，保证速度与高保真

  ![1756994880923](深度学习.assets/1756994880923.png)
  
  | 模型                    | AR 部分                                    | NAR 部分                                             | 优点                                                         | 应用场景                              |
  | ----------------------- | ------------------------------------------ | ---------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------- |
  | **CosyVoice 1.0 / 2.0** | LLM 自回归生成 **speech tokens**           | Flow Matching (tokens→Mel) + HiFi-GAN (Mel→waveform) | 兼顾 **自然度（AR）** 与 **速度（NAR）**；支持情感/风格/语言可控 | 多语言 TTS、跨语言语音克隆、对话式 AI |
  | **VALL-E / VALL-E X**   | GPT 风格模型 → 自回归生成 **codec tokens** | Vocoder 并行解码到 waveform                          | 支持 **few-shot / zero-shot 克隆**；保留说话人音色           | 跨语言语音克隆、个性化 TTS            |
  | **F5-TTS (Meta, 2024)** | AR LLM-like 模型生成 codec tokens          | Diffusion/Flow 解码 → waveform                       | 结合 **对齐友好 + 高保真**，支持多说话人                     | 多语言 / 高质量语音合成               |
  | **MaskGCT**             | 可用 AR 或 Masked AR 生成语音 token        | Flow Matching/Diffusion → Mel                        | **Mask-based** 灵活生成，能处理缺失输入                      | 可控 TTS、鲁棒性任务                  |
  | **VITS / VITS2**        | 隐式 AR（时长/对齐建模）                   | Flow + GAN 并行解码                                  | 高质量端到端，**近似混合**                                   | 研究/工业落地较多                     |

## 主流 TTS 声学模型

---

### Grad-TTS（非自回归）

![1757934015069](深度学习.assets/1757934015069.png)

### Voicebox（非自回归）

![1757941236142](深度学习.assets/1757941236142.png)

- 

## TTS常用指标

---

- **`码本：`**码本是由一组代表向量（码字）组成的集合，用于将输入矢量映射到最近的码字。（通常是在speech feature转化为离散的speech token时使用）目的是压缩speech（因为一般的语音采样为16k、25k则在1s中采样的数据太多，所以需要压缩）。码本越大，每个输入向量被映射的“代表”越精确，量化误差越小，恢复质量越高。

一般通过聚类（k-means等）的方式获得：

1. 准备训练样本集，包含大量输入矢量。
2. 设定码本大小 KKK（即聚类中心数）。
3. 利用K-Means算法将数据划分成 KKK 个簇。
4. 每个簇的中心（均值向量）作为码本中的一个码字。
5. 训练完成后，所有码字组成码本。

- **`MOS（Mean Opinion Score，平均意见分数）：`**评估语音质量的标准指标，通常通过听众对语音的自然度、清晰度和理解度进行评分，评分范围从1到5（1表示非常差，5表示非常好）。

  ![1756627452807](深度学习.assets/1756627452807.png)

- **`NMOS（Normalized Mean Opinion Score）：`**对传统 **MOS（Mean Opinion Score）** 评分的标准化版本，旨在解决在不同设备、测试条件或评审员之间存在的评分偏差问题。通过标准化过程，NMOS可以使得不同测试之间的评分更具有可比性。

  ![1756627499521](深度学习.assets/1756627499521.png)

  ![1756627789822](深度学习.assets/1756627789822.png)

- **`WER（Word Error Rate，词错误率）：`**WER计算的是生成文本与目标文本之间的词语错误数量，并通过这些错误的数量来评估语音识别系统或语音合成系统的性能。WER 值越低表示生成文本与目标文本的相似度越高，即语音识别或语音合成的性能越好。

  ![1756627605257](深度学习.assets/1756627605257.png)

- **`SS、SIM（Speaker Similarity，说话人相似度）：`**衡量两个语音信号或语音样本在说话人特征上的相似度的指标。使用计算得到的相似度值（例如余弦相似度）作为 **Speaker Similarity** (SS)，值越大，表示生成的语音与目标说话人语音越相似。

  ![1756628167955](深度学习.assets/1756628167955.png)

- **`SID（speaker identification，说话人识别）：`**语音处理领域的一个任务，目的是识别一段语音信号来自哪个说话人。

- **`CER（Character Error Rate，字符错误率）：`**一种常用于评估文本生成系统（如语音识别、手写识别等）性能的指标。它衡量的是生成的文本与参考文本之间的字符级错误，通常用于衡量生成的文本的准确性。与 **WER（Word Error Rate，词错误率）** 不同，CER 是基于字符级别的错误率，适用于处理较短的文本或字符级错误对比。

  ![1756628590489](深度学习.assets/1756628590489.png)

- **`：`**

- **`：`**

## TTS 模型

---

### Tacotron2 （seq2seq）

**基本原理：**

> 端到端文本到语音（TTS）系统，主要分为两个阶段，第一阶段是声学模型，第二阶段是声码器。
>
> - 第一阶段：输入文本序列，输出Mel频谱。
>
>   - **文本嵌入层（Embedding layer）**
>      把字符/音素序列映射到稠密向量。
>   - **Encoder**
>      卷积层（CNN） + 双向 LSTM → 把文本转化为高层语义表示。
>   - **Attention 机制**（基于 Location-Sensitive Attention）
>     - 学习“当前时间帧应该对应输入文本的哪个部分”
>     - 解决 TTS 的对齐问题（text 和 audio 时序不一样长）
>     - 使用Attention计算注意力权重（目的就是为了解决Decoder输入（让句子不同的字符输入时的权重不同））
>   - **Decoder（自回归 LSTM）**
>     - 逐帧预测 Mel 频谱
>     - 上一帧的预测作为输入，直到生成完整的频谱
>
>   ⚠️ 注意：这里的 **ground truth Mel 频谱** 会在训练时作为 teacher forcing 的输入，指导 decoder 学习。
>
> - 第二阶段：将Mel频谱转化成语音波形。

### Matcha-TTS（OT-Flow Matching18M）

**最终是将`Mel谱`使用`HIFINET`进行重构成语音波形**

![1759126839240](深度学习.assets/1759126839240.png)

1. 读取数据集中的text文本，Pipeline(English text)：text→ASCII→转成小写→展开缩写→转成音素→去除括号、空白等→音素字符串
   - 完全可以使用`G2P代替`上述过程（优点：最简洁且效果通常更好）
   - 上述过程中得到的音素字符串，相邻的`音素之间插入0`，以区分音素的边界（防止影响发音）
   - 在*collate_fn*函数中，`将每个样本的Mel、text音素长度读取`，`填充每个样本的长度使之对齐`
2. 模型处理时`使用掩码的方式将数据padding部分掩码`，以防padding部分在计算中参与进来，影响模型精度。
   - 使用sequence_mask方法生成每个样本对应的x_mask，使用x_mask * x将padding部分掩码
   - 在模型处理中（encoder、pro_j等）通通使用x_mask进行掩码padding部分（防止影响预测精度）
3. 经过encoder、Duration、Decoder

### cosyvoice2.0（Flow Matching）

**最终是将`Mel谱`使用`HIFINET`进行重构成语音波形**

**模型框架：**

> CosyVoice2.0是5个模型的组合。分别是: 1. 语音分词模型(音频离散化)。2. 将语音转化为离散speech tokens。3. 自回归语言模型(根据输入的text tokens和参考的speech生成speech token)。4. unet-transformer结构的flow-matching (根据生成的speech token （要合成的speech tokens）生成 梅尔频谱)。5. HIFTNET：生成对抗网络，负责将梅尔频谱图转换为语音。

| 模块                                | 训练过程（虚线部分体现）                                     | 推理过程                                                     |
| ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **(a) Supervised Speech Tokenizer** | - 输入语音 XX- Encoder₁ → 压缩 (FSQ-down)- Round 离散化 → 拉长(FSQ-up)→Speech Tokens- Encoder₂ + ASR Decoder 用真实标签 YY 做监督训练 (虚线) | - 输入语音 XX- Encoder₁ → FSQ-down → Round → Speech Tokens，只输出 Speech Tokens（Encoder₂ / ASR Decoder 不用） |
| **(b) Text-Speech Language Model**  | - 输入 **文本 tokens + speech tokens**- 在训练时，真实的目标序列（Output Targets）通过虚线输入，用来计算损失- 虚线中的灰色 Ignore Token 用于掩蔽，不计入损失 | - 输入 **文本 tokens（参考文本tokens和要合成文本 tokens）** 和 **参考语音 tokens**- 模型一步步预测下一个 **speech token（要合成的语音tokens）**，直到生成结束符 E |
| **(c) Chunk-aware Flow Matching**   | - 输入**真实语音 tokens** + **预测合成语音 tokens**- Causal Transformer Encoder, Upsampling Transformer, UNet 通过 Flow Matching 学习分布- 损失由预测与真实语音对齐**（对真实Mel谱随机掩码70%-100%）** | - 输入 **生成的 speech tokens** + **reference tokens**- Causal Transformer Encoder → Upsampling → UNet- 直接生成最终的语音频谱/波形**（将参考语音的Mel谱加入模型帮助合成语音）** |

![1756175672566](深度学习.assets/1756175672566.png)

#### Text Tokenizer

> **基本原理：**
> 在自然语言处理中，tokenizer的作用是将一个`文本序列`通过一个字典转化为一个`token id`的序列。我们回顾图片分类任务，模型在预测的时候，实际上预测的是类别对应的id，而不是类别本身。tokenizer做的事情就是提供一个类似于从类别到对应id的字典。
>
> **代码：**
> 在`processor.py中的tokenize()函数`调用`千问大模型中的get_qwen_tokenizer(预训练的pretrained_models/CosyVoice2-0.5B模型)`，得到`text tokens`。

> **分类：**
>
> 包括两种类型：`字符char、音素Phoneme`
>
> > ==字符char==
> >
> > `1.Word tokenizer：`
> > 给定一个文本序列, 我们现在需要将其转化为一个token序列。一个比较自然的想法是, 我们按照空格将序列拆分成若干个单词, 这样每个单词的语义都能比较好地保留。下面是一个例子：
> >
> > ```python
> > import tiktoken
> > 
> > tokenizer = tiktoken.get_encoding("gpt2")
> > indices = tokenizer.encode("hello world")
> > # indices = [31373, 995]
> > # decode = ["hello", " world"]
> > ```
> >
> > **word tokenizer的缺点为：**
> >
> > 1. 单词数量很大，很多罕见单词的出现频率很低，降低了tokenizer的利用率
> > 2. 对于不在词典内的单词只能用`<UNK>` token表示，损害了语义信息
> >
> > 
> >
> > `2.Character tokenizer：`
> > Character tokenizer的基本思想是使用字符而不是单词来编码文本序列。其实现方式如下：
> >
> > ```python
> > class CharacterTokenizer:
> > def encode(self, s: str) -> list[int]:
> >   return list(ord(c) for c in s)
> > 
> > def decode(self, token_ids: list[int]) -> str:
> >   return "".join(chr(token_id) for token_id in token_ids)
> > ```
> >
> > character tokenizer的词表大小取决于我们的编码方式，UTF-8的编码大概有[110K code points](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/UTF-8)。character tokenizer的缺点总结如下：
> >
> > 1. character tokenizer会导致我们的词表非常大
> > 2. 和word tokenizer一样，很多character非常罕见，会降低词表的利用率
> > 3. token序列的上下文语义信息较差
> >
> > 
> >
> > `3.Byte tokenizer`
> > 所有的字符(character)都是由byte组成的, 比如对于UTF-8编码来说, 每个字符由1-4个byte组成。因此, 所有满足UTF-8编码的文本, 我们都可以将它们转换为基于byte的token序列。由于现在的大部分文本都是基于UTF-8的, 因此, 我们只讨论UTF-8编码的文本。
> >
> > ```python
> > class ByteTokenizer:
> > def encode(self, s: str) -> list[int]:
> >   return list(s.encode("utf-8"))
> > 
> > def decode(self, token_ids: list[int]) -> str:
> >   return bytes(token_ids).decode("utf-8")
> > ```
> >
> > byte tokenizer的问题为：
> >
> > 1. 产生的token序列过长，增加了transformer的计算量
> > 2. 没有上下文语义信息
>
> > ==音素Phoneme==
> >
> > `1.G2P（Grapheme-to-Phoneme）`
> >
> > 通常涉及将 **字符级文本** 转换成 **音素序列**
> >
> > ```python
> > # 安装库
> > pip install g2p-en
> > 
> > # 导入库
> > from g2p_en import G2p
> > 
> > # 创建 G2P 转换器
> > g2p = G2p()
> > 
> > # 输入文本
> > text = "hello"
> > 
> > # 使用 G2P 转换文本为音素
> > phonemes = g2p(text)
> > 
> > # 输出音素序列
> > print(f"输入文本: {text}")
> > print(f"音素序列: {phonemes}")
> > 
> > # 输出结果
> > 输入文本: hello
> > 音素序列: ['HH', 'AH0', 'L', 'OW1']
> > ```
> >
> > 
>

**使用最多的BPE(Byte-Pair Encoding) tokenizer：**

[GPT tokenizer ](https://zhuanlan.zhihu.com/p/714899440)

> 实际生活中，对于出现频率比较高的词，我们会有一个简写的方式，也就是我们使用一个新的单词来表示这个词。比如在英语中，我们会使用`plz` 来代替 `please` 以及使用`how r u` 来代替`how are you`。
> BPE，即byte pair tokenizer的原理也是类似的，对于出现频率比较高的byte pair或者character pair, 我们会使用一个新的token来表示这个pair，这样就压缩了sequence的长度。
>
> BPE算法包括以下几个步骤：
>
> 1. 对文本序列进行pre-tokenize，分割成不同的单词
>
> 2. 当`len(vocab) < vocab_size`时，重复以下步骤：
>
> 3. 1. 对所有单词，统计其相邻character或者byte pair的频率
>    2. 计算出现频率最高的pair，使用一个新的token来表示这个pair
>    3. 将新的token和其对应的`token_id`加入到`vocab`中，并更新单词的分割表示

#### Speech Tokenizer

> **基本原理：**
> 简单来说，这个模型就是使用SenseVoice-Large ASR模型并在其中套入了FSQ（有限标量量化）首先将连续的语音信号声学特征（Mel谱）转换成离散的tokens，也可以通过后续的Encoder2和ASR Decoder模块检测合成语音的效果（通过生成的text内容，计算WER，CER等指标）
>
> **代码：**
> 在`run.sh中的stage=2`时，使用`speech_tokenizer_v2.onnx（SenseVoice-Large预训练模型）`先将训练语音的`Speech tokens提取`出来保存。
> **SenseVoice-Large开源地址：**[FunAudioLLM/SenseVoice: Multilingual Voice Understanding Model](https://github.com/FunAudioLLM/SenseVoice)
>
> - **训练过程（整个图）：**将Mel谱通过Encoder1得到高维特征，通过FSQ-down降成低维特征进行Round有界量化h~i~ ，通过将量化后的低秩表征h~i~ 转换为(2K+1)进制索引，即可获得语音标记μ ~i~。然后通过FSQ-up将低维特征映射到高维，最后通过Encoder2和ASR Decoder得到语音对应的文本。损失通过预测的text与目标text计算。
>
> ![1756208862248](深度学习.assets/1756208862248.png)
>
> - **推理过程（图中绿色的部分）：**将**参考语音**的Mel谱通过Encoder1得到高维特征，通过FSQ-down降成低维特征进行Round有界量化，最终得到参考语音的speech tokens（`注意这里不是要合成的语音tokens`）

![1756208405549](深度学习.assets/1756208405549.png)

#### Unified Text-Speech Language Model

> **基本原理：（其实模型输入输出都是text_token_emb和speech_token_emb，只不过`输入时将tokens转化emb`，`输出时将emb转化为tokens`）将streaming和Non-streaming合成统一到一个模型，streaming和non-streaming唯一不同就是序列结构不同。**
>
> 1. 使用预训练的LLM：使用llm.py中的Qwen2Encoder类调用transforms中的Qwen2ForCausalLM>Qwen2Model>embed_tokens编码器`将上述产生的text tokens转化成text_token_emb`。
> 2. 使用torch.nn.Embedding编码器`将上述产生的speech tokens转化成speech_token_emb`。
> 3. [构建输入]构建两种序列流式和非流式，流式：利用text tokens和speech tokens的比例（论文中是5:15）`指导`text_token_emb和speech_token_emb进行拼接（拼接方式： “文本 → 语音 → 文本 → 语音...”(embedding)）；非流式：直接将text_token_emb和speech_token_emb进行拼接（所有text_token_emb+所有speech_token_emb）。
> 4. [构建目标输出] 流式：如果下一个token是text token那么指导目标输出为`Filling Token`，如果当前输入是speech token则指导目标输出有效的`speech token`，检测到结束符E时，输出`E`和`Ignore Token`；非流式：text tokens指导目标输出全为`Ignore Token`，speech tokens指导目标输出有效（即输出的`speech tokens`数目）,检测到结束符E时，输出`E`和`Ignore Token`。
>
> 
>
> - **训练过程：**使用真实的文本的text tokens和真实的语音的speech tokens，将全部的text tokens和全部的speech tokens输入生成后面的预测speech tokens，然后利用预测的speech tokens和真实的speech tokens计算loss。
> - **推理过程：**利用**参考text tokens**、**参考speech tokens**和**要合成的text tokens**自回归的生成**要合成的speech tokens**

![1756350944399](深度学习.assets/1756350944399.png)

![1756295183222](深度学习.assets/1756295183222.png)

> **补充知识：**
>
> > **自回归（autoregressive, AR）**：按顺序一个一个地预测，每一步的输出依赖于之前已经预测过的结果。这种方式一般用在前后关联比较密切的序列中（`由于语音是 强序列依赖 的：后一帧往往和前一帧的音素/韵律相关，所以也采用这种方式`）
> > **非自回归（No-Autoregressive，NAR）**：不依赖前面预测结果，而是并行预测整段序列。换句话说，模型不是一步步“接龙”，而是一次性把所有 **speech tokens** 都输出出来。
> >
> > 举例：
> > 假设我们要生成 “你好” 这段语音，编码器把它压缩成 5 个 token：
> >
> > [128, 542, 77, 999, 305]
> >
> > 自回归生成过程：
> >
> > 1. 模型先预测第 1 个 token → 128
> > 2. 用 `128` 预测第 2 个 token → 542
> > 3. 用 `[128, 542]` 预测第 3 个 token → 77
> > 4. 用 `[128, 542, 77]` 预测第 4 个 token → 999
> > 5. 用 `[128, 542, 77, 999]` 预测第 5 个 token → 305
> >     最后把 `[128,542,77,999,305]` 送入解码器 → 合成音频。
>
> > **流式合成（Streaming TTS）**：模型在接收到**一部分文本**时，就能**边生成语音边输出播放**，而不是等整段文本处理完才一次性输出整段语音。`交互式场景（语音助手、对话机器人）里，用户不可能等 2–3 秒，流式能在几百毫秒内开始播音。`
> > **非流式合成（Non-Streaming TTS）**：**非流式语音合成**就是指 **必须等到整段文本输入完毕，模型才能一次性合成完整语音**，然后再播放。
> >
> > 举例：
> >
> > 明天的天气是...
> >
> > 非流式 TTS：要等整个句子输入完，模型生成完整音频，再开始播放。
> > 流式 TTS：用户输入“明天的天气…”时，模型就已经生成并播放 “明天的天气” 的声音，等输入完 “是…” 再补后面的语音。

#### Flow Matching

> **基本原理：**
>
> 首先将Reference Tokens（已知有标记的）和Input Tokens（目标合成的）输入到通过前视卷积和两个因果Transformer对齐speech Tokens的representation以**匹配Reference Tokens的声学特征（Mel谱）**。Flow Matching主要在causal convolutional Transformer UNet网络中，在该网络中以上采样后的speech Tokens、掩码的Mel谱图、说话人embedding和时间步作为条件，利用Flow Matching将已知的高斯分布X~0~推到目标Mel谱的X~1~。
>
> - 训练阶段：Input Tokens（预测的、合成的语音tokens）和Reference tokens（真实的语音tokens）通过三个对齐模块得到上采样的speech Tokens **μ**，采用3D-Speaker模型中的ERes2Net得到speaker embedding **v**。将以上得到的**v** 、**μ**和真实的Mel谱图随机掩码70%-100%输入至Unet训练（将真实语音Mel谱X~1~作为目标分布，随机初始化的高斯噪声作为输入）。时间步t是在[0, 1]上的均匀分布。损失通过Unet预测的向量场方向和目标分布X~1~ - X~0~的平方差计算。
> - 推理阶段：与训练阶段不同的是，将**Reference Tokens（参考语音的tokens）Mel谱作为目标输入，时间步t是采用余弦调度器获得更多的时间步。

![1756435669102](深度学习.assets/1756435669102.png)

**Flow Matching的基本原理：**Flow Matching 的目的是让模型学会一个流场（vector field），这个流场定义了数据从起点（比如噪声分布）到终点（目标分布）如何逐步变化的方式。
[(6 封私信 / 50 条消息) 通俗易懂理解Flow Matching - 知乎](https://zhuanlan.zhihu.com/p/16113190076)

- **时间步长：**Flow Matching 将生成过程拆分成多个时间步长，每一步都模拟从一个分布“流动”到下一个分布。
- **损失函数：**Flow Matching 的损失函数会比较模型生成的路径（实际流动）和目标路径（理想流动）之间的差距。

**训练数据的生成：**
目的是为了训练出一个向量场即上方提出的流场。

1. 需要初始分布的数据样本X~0~（符合高斯分布的随机噪声）
2. 需要目标分布的数据样本X~1~（目标，清晰的Mel谱图）
3. 需要中间状态的数据X(t)（代表着从初始分布到目标分布的插值，常见的插值法有**随机插值**和**线性插值**）

**训练模型：**
定义正确的流动方向：X~1~ - X~0~的方向

![1756558915905](深度学习.assets/1756558915905.png)

让模型训练的向量场和X~1~ - X~0~尽可能一致，则会有以下的损失函数：通过计算模型预测的向量场方向与目标方向的差值作为loss。

![1756559015612](深度学习.assets/1756559015612.png)

- **线性插值：**
  每一步的中间状态X(t)都是初始状态X~0~和X~1~的加权平均。

![1756556988150](深度学习.assets/1756556988150.png)

- **随机插值：**
  如果初始和目标分布比较复杂（比如高维图像空间），可以加入噪声让中间状态更接近实际分布。

![1756557221823](深度学习.assets/1756557221823.png)

如下图所示，其中X~1~是目标分布，X~0~是初始的高斯分布，两者的差值即为理想的速度方向。让模型学到向量场 **v**让其和理想的速度方向相同。φ~t~上标OT是指最优传输（optimal-transport），它是X~0~和X~1~的中间状态X~t~，其中 t 属于[0,1]之间，t=0时φ~t~=X~0~，t=1时φ~t~=X~1~（`该方法叫做线性插值法`）本质就是让左边的模型预测方向=右边的理想方向（下图第一行公式）。

![1756558165281](深度学习.assets/1756558165281.png)

**掩码操作：**
为了适应**流式合成**模型采用了4种掩码方式：

方式：将多步流估计视为一个堆叠的深度神经网络（十层的Unet模型），以适应流式合成的任务。

**四种掩码方式：**

- **Non-causal Mask**：用于离线模式，可以达到最佳性能，因为它能够`访问所有条件帧`。适用于对延迟不敏感的应用场景。

- **Full-causal Mask**：设计用于需要极低延迟的场景，其中`只能访问过去的帧，无法访问未来的帧`。

- **Chunk-M Mask**：在延迟和性能之间进行权衡，`能够同时利用过去的帧和M个未来的帧`。适合用于生成过程的第一块（chunk），能够较好地控制延迟。

- **Chunk-2M Mask**：通过牺牲一定的延迟，`能够同时利用过去的帧和2M个未来的帧`接近离线模式的性能。这种掩膜适合于级联生成任务，提供更好的性能。

![1756624910527](深度学习.assets/1756624910527.png)

**补充知识：**

> > **链接：**[上采样？下采样？大白话理解_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1St421w7fD/?spm_id_from=333.1391.0.0)
> >
> > **上采样：**
> >上采样通常指的是将低采样率或低分辨率的音频特征（如梅尔频谱图）转化为高采样率或高分辨率的音频特征的过程，最终生成更高质量的音频信号。（通俗的理解就是在离散的样本点之间插入新的样本）
> > 
> >**下采样：**
> > **下采样（Downsampling）** 是指将音频信号或其特征图的分辨率降低的过程。这通常是为了减少计算量、提取更高级的特征或者将音频信号的时间分辨率降低，从而减少网络的复杂度和存储需求。（通俗的理解就是在离散的样本点之间删除一个样本点）
>
> > **Flow matching：**目标是找到一条“平滑、正确”的路径，把起点的随机噪声逐步变成目标分布。
> >
> > **Diffusion：**可以分为一个前向过程和后向过程，前向过程是将真实图片经过一系列过程转化成高斯（噪声）图片；后向过程是将噪声图片转化成真实图片过程。（类似于出一个问题，然后求解一个问题）
> >
> > - **Normalizing Flows(NFs)：**是一种可逆的概率密度变换方法，它的核心思想是通过一系列可逆的变换函数来逐步将一个简单分布（通常是高斯分布）转换成一个复杂的目标分布。
> > - **Continuous Normalizing Flows (CNFs)：** 在**CNFs**中，这种变换是**连续的**，这使得模型能够更加平滑地适应数据的分布，提高了模型的表达能力。**CNFs**过程通过常微分方程（**ODE**）来表示
> > - Diffusion 模型本质是：正向过程是一个简单的 SDE（加噪声），逆过程是另一个 SDE（去噪声）。随机微分方程**（SDE）**
>
> > **Unet：**
> >
> > **关键特性：**
> >
> > **1.编码器-解码器结构**：
> >
> > - **编码器（Contracting Path）**：由一系列卷积层和池化层组成，用于提取图像的特征。在这个过程中，图像的空间分辨率逐渐降低，`捕捉到越来越抽象的特征`。
> > - **解码器（Expansive Path）**：通过一系列上采样（或反卷积）操作逐步恢复图像的空间分辨率，最终生成与输入图像相同尺寸的输出。解码器的目的是`将提取到的高层次特征转化回原始图像的尺寸`。
> >
> > **2.跳跃连接（Skip Connections）**：
> >
> > - 在U-Net中，编码器和解码器之间有直接的跳跃连接。这些连接将编码器中的特征图传递到解码器中相应位置，`帮助解码器恢复细节`，特别是在空间位置和边缘信息方面。跳跃连接有助于避免在下采样过程中丢失重要的位置信息，从而提高分割结果的精度。
> >
> > **3.对称结构**：
> >
> > - U-Net的结构是对称的，即编码器和解码器的层数和结构相似，确保了从低级特征到高级特征的有效传递和恢复。对称性使得模型能够`有效地学习不同层级的特征`。
> >
> > ![1756621960408](深度学习.assets/1756621960408.png)
>
> >**卷积的本质：**
> >
> >卷积的本质是通过在输入数据上进行局部加权求和的操作来提取特征。这使得卷积神经网络能够在处理图像、语音等数据时，逐层提取越来越复杂的特征，并通过**权重共享**和**局部感知**提高效率和性能。卷积操作通过有效地减少计算量和参数数量，是深度学习模型特别是在图像处理领域成功的关键。

### MaskGCT（掩码生成1.6B）

==最终是将`Acoustic tokens`使用`Speech Acoustic Codec`重构成语音波形==

#### 组成原理

MaskGCT是一个`完全非自回归（NAR）的Zero-shot-TTS`模型，其关键在于`掩码（mask）和二阶段`。整个模型结构如下图所示，最主要的两个部分如下两框所示。第一阶段`Text-to-Semantic MaskGCT`和第二阶段的`Semantic-to-Acoustic MaskGCT`的过程如下图和下表所示。

**整体结构：**

1. 使用`Speech Semantic Codec`将**Prompt Speech**转化成**Prompt Semantic Tokens**
2. 使用`Text-to-Semantic MaskGCT`将**Prompt Semantic Tokens、Text**转化成**Semantic Tokens**
3. 使用`Semantic-to-Acoustic MaskGCT`将**Semantic Tokens**转化成**Acoustic Tokens**
4. 使用`Speech Acoustic Codec`将**Acoustic Tokens**转化成**语音波形**

![1758633295264](深度学习.assets/1758633295264.png)

**Text-to-Semantic MaskGCT和Semantic-to-Acoustic MaskGCT结构：**

![1758634065275](深度学习.assets/1758634065275.png)

|                 阶段                  |                          输入 /条件                          |                          输出 /任务                          |                           关键特点                           |
| :-----------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|   **阶段1：Text-to-Semantic (T2S)**   | 文本 + prompt 的Semantic token（从语音→SSLfeature→VQ（向量量化codebook→semantic tokens） 取部分semantic tokens作为Prompt，剩下的部分mask掉 | 预测被mask掉的对应 **Semantic token** 序列，其中部分 token 被 mask，需要恢复这些被遮蔽的 token（mask-and-predict 风格） | 不需要显式的文本-音频对齐 (alignment) 信息；不需要 phone-level duration 预测；可以控制生成语义 token 的长度。 |
| **阶段2：Semantic-to-Acoustic (S2A)** | 上阶段生成的Semantic token + prompt 的Acoustic token+Mask 的Acoustic token（Acoustic token是`SoundStorm的Encoder和VQ处理训练语音得到的`。（与Semantic token码本长度相等），Acoustic token分为target Acoustic token和Prompt Acoustic token） | 生成被mask掉的acoustic tokens，这些 token 再由声码器 codec 解码成为语音波形 | 同样使用 mask-and-predict；条件依赖语义 token；无对齐 supervision；并支持长度控制等特性。 |

**Speech Semantic Codec 和 Speech Acoustic Codec结构：**

1. **Speech Semantic Codec：**训练VQ-VAE模型学习向量量化码本（主要是将W2v-BERT 2.0得到的Semantic features进行重构）。其中Encoder将Semantic features编码为S，然后将S量化为E（将高维量化为低维），最后通过Decoder将E重构成S~1~
2. **Speech Acoustic Codec：**Encoder将audio压缩编码，通过训练RVQ码本，最后使用类似Vocos的Decoder构建语音波形。

![1758634142090](深度学习.assets/1758634142090.png)

### NaturalSpeech 3（diffusion）

# 声纹识别

---

**基本原理:**

`声纹识别（Voiceprint Recognition）是通过分析语音信号中的生物特征（如声道结构、声带振动）和行为特征（如发音习惯、语速、韵律）进行身份认证的技术。其核心假设是：每个人的发声器官（唇、舌、喉等）具有唯一性，且说话习惯难以完全模仿。`

**与语音识别区别：**

|          |     声纹识别     |      语音识别      |
| :------: | :--------------: | :----------------: |
|   目标   | 识别说话人的身份 |  识别说的具体内容  |
| 特征提取 |  提取说话人特征  | 提取语句的具体内容 |
| 模型输出 |    说话人的ID    |     文本、指令     |
| 应用场景 |     安全认证     | 字幕翻译、语音助手 |

**输入特征：**

常用的**输入特征**为`Mel谱、频率谱和Fbank谱`

**特征提取：**

**1.MFCC特征提取过程及作用：**
（1）预处理：预加重（提升高频）、分帧（20-40ms/帧）、加窗（Hamming，帧边缘的**频谱泄漏**）
（2）STFT（FFT）：对于每一帧做傅里叶变换得到频谱（包括幅度和相位）,一般都是对得到的复数取绝对值得到幅度谱（去除相位）→ 频谱（幅度谱）
（3）梅尔滤波：将线性频率映射到**Mel尺度**（过程包括将频谱转化为Mel、处理、Mel转化为频谱）→ 梅尔谱
（4）取对数：动态范围压缩 → Fbank谱
（5）DCT：对对数能量做离散余弦变换，保留前12-16维系数 → MFCC谱
作用：压缩冗余信息，保留说话人特征；梅尔尺度更贴合人耳感知 

**2.Fbank特征提取：（除了MFCC的DCT过程没有，其他步骤相同）**
（1）预处理：预加重（提升高频）、分帧（20-40ms/帧）、加窗（Hamming）
（2）STFT（FFT）：对于每一帧做傅里叶变换得到频谱
（3）梅尔滤波：将线性频率映射到**Mel尺度**
（4）取对数：动态范围压缩

**3.DCT（离散余弦变换）作用：**
DCT的实质是去除各维信号之间的相关性，将信号映射到低维空间。

**4.MFCC和Fbank特征提取的区别：**

|            |                  Fbank                   |      MFCC       |
| :--------: | :--------------------------------------: | :-------------: |
|    维度    | 保留高维特征（可以保留语音更多原始特征） |    低维特征     |
| 计算复杂性 |                   较低                   | 需要经过DCT变换 |
|    应用    |             端到端的深度学习             |  传统的GMM-HMM  |

**特征的种类：**
通过网络或者概率技术，提取到的可以表示语音的向量。

|          |         技术         |     输入      | 典型的维度 |        优势        |           劣势           |
| :------: | :------------------: | :-----------: | :--------: | :----------------: | :----------------------: |
| i-vertor |     概率统计模型     |     MFCC      |  400-600   |  小规模数据、低噪  |      对信道变化敏感      |
| d-vertor | 深度神经网络（DNN）  |  Fbank、MFCC  |  512-1024  |    固定文本场景    | 依赖固定文本、泛化能力弱 |
| x-vertor | 时延神经网络（TDNN） |  Fbank、MFCC  |    512     | 长语音、大规模数据 |      短语音性能有限      |
| r-vertor |     端到端的网络     | 原始频谱/波形 |  256-512   |     短语音高噪     |       计算复杂度高       |



## 语音鉴伪

---

**基本原理：**

`语音鉴伪的核心目标是检测语音是否被伪造或篡改（如AI合成、语音克隆、剪辑拼接等），其基本原理是通过分析语音信号中的物理特征、内容一致性及发音行为模式，寻找人工合成或篡改难以复制的自然人类语音痕迹。`

### 伪造语音数据集

---

- **完全伪造的语音数据集：**`ASVspoof2019-LA、ASVspoof2021-LA、ASVspoof2021-DF、FakeorReal-orginal (FoR)、WaveFake、In-the-Wild (ITW)、TIMIT-TTS、FMFCC-A、Chinese Fake Audio Detection (CFAD)、ADD2022-LF、Latin-American Voice Anti-spoofing、Multi-Language Audio Anti-spoofing (MLAAD)`
- **部分伪造的语音数据集：**`Partial Synthetic Detection (Psynd)、PartialSpoof、ADD2022-PF、ADD2023-PF、Half-Truth (HAD)`
- **完全真实的语音：**`Voice cloning toolkit (VCTK)、LibriSpeech、VoxCeleb2、LJ Speech、AISHELL-3`

### ASVspoof5

---

#### **比赛：**

- 赛道1：CM（伪造检测）系统（包括close（不使用ASVspoof5之外的预训练模型和数据集）和open（可以使用预训练模型和数据集且数据集不能和ASVspoof5数据集重合））
- 赛道2：SASV（鲁棒伪造自动说话人验证）系统（和CM要求一样）

![1753601664480](深度学习.assets/1753601664480.png)

#### **数据集（Track1）**

- **训练集：**其中的欺骗性数据使用的攻击算法均使用`零样本TTS`形式，

  - **A01:** 基于Glow-TTS 的 TTS 系统，使用ECAPA-TDNN说话人编码器

  - **A02:** 基于Glow-TTS 的 TTS 系统，使用ResNet-34说话人编码器

  - **A03:** 基于Glow-TTS 的 TTS 系统，使用TDNN-Y说话人编码器

  - **A04:** 基于Grad - TTS的TTS系统，使用ECAPA-TDNN说话人编码器

  - **A05: **基于Grad - TTS的TTS系统，使用ResNet-34说话人编码器

  - **A06:** 基于Grad - TTS的TTS系统，使用TDNN-Y说话人编码器

  - **A07:** 基于Fast Pitch的TTS系统，使用ECAPA-TDNN说话人编码器

  - **A08:** 基于Transformer的vits系统

    ![1753163264525](深度学习.assets/1753163264525.png)

- **验证集：** 包含了由零样本和少样本TTS和VC系统混合产生的攻击。

  - **A09:** 使用语音合成包 IMS Toucan（零样本TTS技术）
  - **A10:** 与A09类似
  - **A11:** 基于Tacotron 2的零样本合成
  - **A12:** 使用少量样本加上预训练模型使用Toucan生成语音
  - **A13:** 基于StarGAN - ZSVC模型的零样本VC ( ZSVC )系统
  - **A14:** 建立在YourTTS 上的零样本的TTS
  - **A15:** 少量样本的VC技术（语音转换）
  - **A16:** 零样本的VC技术

  ![1753164101299](深度学习.assets/1753164101299.png)

- **测试集：** 数据是使用9个TTS / VC系统和7个对抗攻击生成的。其中的三种TTS攻击，即A17，A28和A29，是使用使用外部数据集预训练的现成系统生成的。

  - A17:
  - A18：
  - ......

  ![1753164141773](深度学习.assets/1753164141773.png)

#### 实验

**所有统计数据均在track1上的数据集**

|                     |  real  |  fake  | total  |
| :-----------------: | :----: | :----: | :----: |
|  **Training set**   | 18797  | 163560 | 182357 |
| **Development set** | 31334  | 109616 | 140950 |
|    **Train&Dev**    | 50131  | 273176 | 323307 |
| **Evaluation set**  | 138688 | 542086 | 680774 |

**计算每个数据集中音频的最大值、最小值和均值**

|                     | min（s） | mean（s） | max（s） |
| :-----------------: | :------: | :-------: | :------: |
|  **Training set**   |   2.61   |   11.92   |  28.91   |
| **Development set** |   0.06   |   7.08    |  22.43   |
|    **Train&Dev**    |   0.06   |   9.81    |  28.91   |
| **Evaluation set**  |   2.00   |   7.09    |  19.37   |



1. 对于`训练数据和验证数据数量相差太小，训练集的数据对验证集几乎不会使验证集的EER变化`，将训练集和验证集合并，同时取出80%作为训练集，20%作为验证集
   - 对于**类别不平衡数据**问题，可以选用分层抽样，在每个epoch中将数据打乱且随机划分训练集和验证集（未实现）
   - 使用数据增强的方法，弥补正样本数量不足（未实现）
   - 使用ASL损失函数的方法，在求损失时分配不同的权重（实现了，但没效果去掉）
2. 对于训练不准确的问题使用`SSL提取特征`主要有以下几种SSL方法
   - WavLM
   - Wav2vec2
   - UniSpeech
3. 

# GPU训练

---

## 单机单卡训练

---

- **定义device：**在训练之前使用如下代码调用torch中的device

  ```python
  device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")#此处可以更改cuda：0/1
  ```

- 对于**模型、数据**使用以下代码（表明使用cuda进行计算）

  ```python
  model = ECAPA_TDNN().to(device)
  #将模型加载到GPU上训练
  data = data.to(device, non_blocking=True)
  #此处的non_blocking和DataLoader中的pin_memory配合使用，可以加快CPU加载数据的速度。
  #训练时data.to("cuda", non_blocking=True)
  #或者使用第3行时用data.to(device, non_blocking=True)
  ```

- **保存和加载模型**

  ```python
  torch.save(self.state_dict(), path)
  #state_dict()：保存模型参数，path：模型保存的路径
  loaded_state = torch.load(path, map_location=torch.device("cuda:1"))
  #path:模型的路径，map_location：使用哪个GPU加载
  ```
## 单机多卡训练（DDP & torchrun）

---

**官网地址：** [Distributed communication package - torch.distributed — PyTorch 2.7 documentation](https://docs.pytorch.org/docs/stable/distributed.html#distributed-basics)

**参考代码：** [ddp-demo/singlenode_gpu.py at main · owenliang/ddp-demo](https://github.com/owenliang/ddp-demo/blob/main/singlenode_gpu.py)

### 检查设备是否可以使用CUDA

```python
# 在虚拟环境终端下输入python进入python中
# 导入torch
import torch
# 输入指令查看,如果返回True说明可以使用CUDA
print(torch.cuda.is_available())
# 查看该机器有多少张GPU
print(torch.cuda.device_count())
```

### 初始化分布式进程组

```python
# 导入库，初始化进程池
from torch.distributed import init_process_group
import torch.distributed as dist

# 【集合通讯】其他进程连master，大家互认
init_process_group(backend="nccl")  # nvidia的GPU/CPU用“nccl”通讯方式
# 自动获取该机器上的GPU设备，并设置为当前设备
local_rank = dist.get_rank()
# 该机器有几台GPU
world_size = dist.get_world_size()
```

### 使用DDP对模型进行包装

```python
# 实现分布式数据并行的核心类  
# DDP 在每个 GPU 上运行一个进程，其中都有一套完全相同的 Trainer 副本（包括model和optimizer）
from torch.nn.parallel import DistributedDataParallel as DDP

device = f'cuda:{local_rank}'
# 将模型放在当前运行的GPU上,使用DDP包装model之前必须先将model放在GPU上！！！
model = model(device).to(device)
# 自动继承模型当前设备,如果设成DDP(model, device_ids=[local_rank])，那么在验证时（只使用rank0）会出错！
model = DDP(model)# 【集合通讯】rank0广播参数给其他进程
```

### 对数据进行打乱分发

```python
# 这个 sampler 可以把采样的数据分散到各个 GPU 上    
from torch.utils.data.distributed import DistributedSampler  

# 这个 sampler 自动将数据分块后送个各个 GPU，它能避免数据重叠
train_sampler = DistributedSampler(dataset)
# 设置了新的 sampler，参数 shuffle 要设置为 False （这里shuffle和sampler是互斥的，不能同时设置）
dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=False, 								sampler=train_sampler) 

# 注意：在每个epoch开始，调用train_sampler.set_epoch(epoch)可以使得数据充分打乱！！！
#  #验证集# 不需要使用sampler打乱，直接加载即可！！！！！！！！！！！！
dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True) 
```

### 训练（使用多卡）

```python
# 开始训练，导入库
import torch.distributed as dist

model.train()
for epoch in range(args.num_epochs):
        # 每个epoch调用该函数，使数据充分打乱
        train_sampler.set_epoch(epoch)
        dev_sampler.set_epoch(epoch)
        # 在进程0中加载进度条
        if local_rank == 0:
            train_loader = tqdm(train_loader, file=sys.stdout)
        for step, data in enumerate(train_loader, start=1):
            ...
            optimizer.zero_grad()
            loss.backward() # 将所有进程梯度汇总并求平均（该loss为单一loss，不是batch的loss）
            optimizer.step()
            # 因为DDP不会自动将loss求均值，所以一般在该位置对每个batch的loss进行汇总求平均
            dist.all_reduce(loss)
            loss = loss.item() / world_size
            # 将每个batch上的均值loss求和
            train_batch_loss +=  loss
        # 求均值loss
        train_batch_loss /= len(train_loader)# len(train_loader)为step=样本总数/batch_size
```

### 验证（使用单卡）

```python
from torch.distributed import  barrier
for epoch in epochs:
    if local_rank == 0:
        model_eval()
        # 将model从DDP中剥离出来，这样就可以在单GPU上验证
        raw_model=model.module
        with torch.no_grad():
        	for x,y in val_dataloader:
            	x,y=x.to(device_name),y.to(device_name)
                pred_y=raw_model(x)
                loss=F.cross_entropy(pred_y,y)
                # 将每个batch_size的平均loss算出来
                val_loss += loss.item()
            # len(dev_loader)返回的是：音频总数/batch_size
            val_avg_loss /= len(val_dataloader)
    # 每个epoch后使用barrier()，让其他GPU等待rank0上验证完验证集
    barrier()
```

### 销毁进程池

```python
# 导入销毁进程池的包
from torch.distributed import destroy_process_group

# 销毁进程池
destroy_process_group()
```

### 终端训练指令

```bash
torchrun --standalone --nproc_per_node=2 XXX.py
# --standalone 代表单机运行
# --nproc_per_node=gpu 代表使用所有可用GPU（会自动查询该设备可用的GPU）
# 如果想要进一步指定要运行的 GPU，可以通过 CUDA_VISIBLE_DEVICES 设置GPU可见性，比如
CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 multi_gpu_torchrun.py --num_workers=8 
```

### 整个流程

```python
# from torch.utils.data.distributed和from torch.distributed完全不同！！！！！
from torch.utils.data.distributed import DistributedSampler        
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import torch.distributed as dist

# 1.初始化进程池
# 【集合通讯】其他进程连master，大家互认
init_process_group(backend="nccl")  # nvidia的GPU/CPU用“nccl”通讯方式
# 自动获取该机器上的GPU设备，并设置为当前设备
local_rank = dist.get_rank()
# 该机器有几台GPU
world_size = dist.get_world_size()


# 2.使用DDP对模型进行包装
device = f'cuda:{local_rank}'
# 将模型放在当前运行的GPU上,使用DDP包装model之前必须先将model放在GPU上！！！
model = model(device).to(device)
# 自动继承模型当前设备,如果设成DDP(model, device_ids=[local_rank])，那么在验证时（只使用rank0）会出错！
model = DDP(model)# 【集合通讯】rank0广播参数给其他进程


# 3.使用DistributedSampler对数据进行包装，充分打乱
# 这个 sampler 自动将数据分块后送个各个 GPU，它能避免数据重叠
##################################### 训练集 ##########################################
train_set = Dataset_train（......）# 继承Dataset类，加载数据集
train_sampler = DistributedSampler(train_set)
# 设置了新的 sampler，参数 shuffle 要设置为 False （这里shuffle和sampler是互斥的，不能同时设置）
dataloader = DataLoader(train_set, batch_size=batch_size, pin_memory=True, 
                        shuffle=False, sampler=train_sampler) 
###################################### 验证集 ##########################################
dev_set = Dataset_dev（......）# 继承Dataset类，加载数据集
dataloader = DataLoader(train_set, batch_size=batch_size, pin_memory=True, shuffle=True) 


# 4.开始训练
for epoch in range(args.num_epochs):
	# 每个epoch调用该函数，使数据充分打乱
	train_sampler.set_epoch(epoch)
    ######################################################## 训练 ##################################################
    # 在进程0中加载进度条
    if local_rank == 0:
        train_loader = tqdm(train_loader, file=sys.stdout)
    for step, data in enumerate(train_loader, start=1):
        ...
        optimizer.zero_grad()
        train_loss.backward() # 将进程梯度汇总并求平均（该loss为单一loss，不是batch的loss）
        optimizer.step()
        
        # 因为DDP不会自动将loss求均值，所以一般在该位置对每个batch的loss进行汇总求平均
        dist.all_reduce(loss)
        loss = loss.item() / world_size
        # 将每个batch上的均值loss求和
        train_batch_loss +=  loss
        
        # 计算正确率
        train_num_correct = torch.eq(batch_pred, batch_y.to(local_rank)).sum(dim=0)
        dist.all_reduce(train_num_correct)
        train_num_correct = train_num_correct.item() / world_size
        train_batch_correct += train_num_correct
    # 求均值loss
    train_batch_loss /= len(train_loader)# len(train_loader)为step=样本总数/batch_size
    # 求正确率
    train_batch_correct /= (batch_size*step)
    train_accuracy = train_batch_correct*100
    ###################################################### 验证 ###################################################
    if local_rank == 0:
        model_eval()
        # 将model从DDP中剥离出来，这样就可以在单GPU上验证
        raw_model=model.module
        with torch.no_grad():
        	for x,y in val_dataloader:
            	x,y=x.to(device_name),y.to(device_name)
                pred_y=raw_model(x)
                loss=F.cross_entropy(pred_y,y)
                # 将每个batch_size的平均loss算出来
                val_loss += loss.item()
            # len(dev_loader)返回的是：音频总数/batch_size
            val_avg_loss /= len(val_dataloader)
    # 每个epoch后使用barrier()，让其他GPU等待rank0上验证完验证集
    barrier()

# 5.销毁进程池
destroy_process_group()
```

## 加载预训练模型

---

### 单卡训练，单卡加载

- **模型保存**

  ```python
   # 保存时将想保存的模型参数、优化器参数等封装成一个字典，方便后期模型加载
   torch.save({'model':model.state_dict(), 'optimizer':optimizer.state_dict()},  './Best_Model.pth')
  ```

- **模型加载**

  ```python
  #先初始化模型，因为保存时只保存了模型参数，没有保存模型整个结构
  model = Model()
  #然后加载参数, checkpoint是cuda:0保存的，加载默认会读到cuda:0，所以明确指定给cpu
  checkpoint = torch.load(pretrain_model_path, map_location='cpu') #pretrain_model_path是你保存的模型文件的位置
  # 加载模型参数
  model.load_state_dict(checkpoint['model'])
  # 加载优化器参数
  optimizer.load_state_dict(checkpoint['optimizer']) 
  ```

### 单卡训练，多卡加载

  - **模型保存**

    ```python
     # 保存时将想保存的模型参数、优化器参数等封装成一个字典，方便后期模型加载
     torch.save({'model':model.state_dict(), 'optimizer':optimizer.state_dict()},  './Best_Model.pth')
    ```

  - **模型加载**

    ```python
    #先初始化模型，因为保存时只保存了模型参数，没有保存模型整个结构
    model = Model()
    #然后加载参数, checkpoint是cuda:0保存的，加载默认会读到cuda:0，所以明确指定给cpu
    checkpoint = torch.load(pretrain_model_path, map_location='cpu') #pretrain_model_path是你保存的模型文件的位置
    # 加载模型参数
    model.load_state_dict(checkpoint['model'])
    # 加载模型参数后使用DDP对模型并行处理！！！
    model = DDP(model)
    # 加载优化器参数
    optimizer.load_state_dict(checkpoint['optimizer']) 
    ```

### 多卡训练，单卡加载（1）

如果在你使用多卡训练时，想着以后用单卡加载，建议保存模型时，去除模型参数里的module。使用**model.module.state_dict()代替model.state_dict()**，优化器不做任何变化。`多卡训练时保存模型尽量将module剥离出来保存`

- **模型保存**

  ```python
   # 保存时将想保存的模型参数、优化器参数等封装成一个字典，方便后期模型加载
   torch.save({'model':model.module.state_dict(), 'optimizer':optimizer.state_dict()},  './Best_Model.pth')
  ```

- **模型加载**

  ```python
  #先初始化模型，因为保存时只保存了模型参数，没有保存模型整个结构
  model = Model()
  #然后加载参数, checkpoint是cuda:0保存的，加载默认会读到cuda:0，所以明确指定给cpu
  checkpoint = torch.load(pretrain_model_path, map_location='cpu') #pretrain_model_path是你保存的模型文件的位置
  # 加载模型参数
  model.load_state_dict(checkpoint['model'])
  # 加载优化器参数
  optimizer.load_state_dict(checkpoint['optimizer']) 
  ```

### 多卡训练，单卡加载（2）

**使用model.state_dict()**保存，但是**单卡加载的时候，要把模型做并行化（在单卡上并行）**`多卡训练时保存模型尽量将module剥离出来保存`

- **模型保存**

  ```python
   # 保存时将想保存的模型参数、优化器参数等封装成一个字典，方便后期模型加载
   torch.save({'model':model.state_dict(), 'optimizer':optimizer.state_dict()},  './Best_Model.pth')
  ```

- **模型加载**

  ```python
  #先初始化模型，因为保存时只保存了模型参数，没有保存模型整个结构
  model = Model()
  #并行处理模型
  model = DDP(model)
  #然后加载参数, checkpoint是cuda:0保存的，加载默认会读到cuda:0，所以明确指定给cpu
  checkpoint = torch.load(pretrain_model_path, map_location='cpu') #pretrain_model_path是你保存的模型文件的位置
  # 加载模型参数
  model.load_state_dict(checkpoint['model'])
  # 加载优化器参数
  optimizer.load_state_dict(checkpoint['optimizer']) 
  ```

### 多卡训练，多卡加载

与 **多卡训练，单卡加载（2）** 类似。`多卡训练时保存模型尽量将module剥离出来保存`

- **模型保存**

  ```python
   # 保存时将想保存的模型参数、优化器参数等封装成一个字典，方便后期模型加载
   torch.save({'model':model.state_dict(), 'optimizer':optimizer.state_dict()},  './Best_Model.pth')
  ```

- **模型加载**

  ```python
  #先初始化模型，因为保存时只保存了模型参数，没有保存模型整个结构
  model = Model()
  #并行处理模型
  model = DDP(model)
  #然后加载参数, checkpoint是cuda:0保存的，加载默认会读到cuda:0，所以明确指定给cpu
  checkpoint = torch.load(pretrain_model_path, map_location='cpu') #pretrain_model_path是你保存的模型文件的位置
  # 加载模型参数
  model.load_state_dict(checkpoint['model'])
  # 加载优化器参数
  optimizer.load_state_dict(checkpoint['optimizer']) 
  ```

### 例子：

- **模型保存**

  ```python
   # 保存时将想保存的模型参数、优化器参数等封装成一个字典，方便后期模型加载
   torch.save({'model':model.module.state_dict(), 'optimizer':optimizer.state_dict()},  './Best_Model.pth')
  ```

- **模型加载**

```python
# 一般放在 <初始化分布式进程组后面>

checkpoint=None # 各自加载checkpoint
try:
    # checkpoint是cuda:0保存的，加载默认会读到cuda:0，所以明确指定给cpu
    checkpoint=torch.load('checkpoint.pth',map_location='cpu')   
except:
    pass

# 因为此处保存的是去除module的，所以放在DDP前面！！！！如果使用多卡训练时使用model.state_dict()保存模型参数，则应放在DDP后
if checkpoint and local_rank==0:  # rank0恢复模型参数
	model.load_state_dict(checkpoint['model'])

# 一般放在优化器初始化后面
if checkpoint:
    optimizer.load_state_dict(checkpoint['optimizer'])  # 各自加载checkpoint
```

![1753537140930](深度学习.assets/1753537140930.png)

## Vscode调试

### 单机多卡

`使用 VScode 的话，可以如下编辑 launch.json 文件，然后像往常一样设置断点按 f5 调试即可`

```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: torchrun",
            "type": "python",
            "request": "launch",
            // 设置 program 的路径为 torchrun 脚本对应的绝对路径
            "program": "/home/hhc/miniconda3/envs/spoof5_baseline/lib/python3.8/ \
            site-packages/torch/distributed/run.py",
            // 设置 torchrun 命令的参数
            "args":[
                "--standalone",
                "--nproc_per_node=gpu",
                "multi_gpu_torchrun.py"
            ],
            "console": "integratedTerminal",
            "justMyCode": true,
            "env": {
                //指定路径
                "PYTHONPATH": "/home/hhc/project/My_project",
                "MASTER_ADDR": "localhost",
                "MASTER_PORT": "29500",
                "RANK": "0",
                "WORLD_SIZE": "1"，
                "LOCAL_RANK": "0"
            },
        }
    ]
}
```

### 单机单卡

```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger",
            "type": "debugpy",
            "request": "launch",
            "program": "/home/hhc/project/My_project/data_utils.py", //调试文件
            "console": "integratedTerminal",
            "env": {
                "PYTHONPATH": "1"
            },
            "args": [
                "--config", "configs/ljspeech.json" //含有参数的加入参数
            ],
            "cwd": "/home/hhc/project/My_project"  // 确保工作目录是项目根目录
        }
    ]
}
```



# 服务器使用

## Linux基本指令

### 拷贝文件：（cp）

```python
cp [options] src des
#options:
#-r：递归到整个目录
#-v：显示详细操作过程
#src:源文件夹路径
#des:目的文件夹路径
```

### 移动、重命名文件

```python
mv  file.txt  file111.txt
#更改文件名
mv  path/file.txt  des_path/
#移动文件到文件夹下
mv  path/src/  path1/des_path/
#移动文件夹到指定文件夹下
```

### 批量处理.txt文件中的字符串（整体替换）

```python
# 将filelists.txt文件中的'DUMMY'字符串全部替换为'ljs_dataset_folder/wavs'
sed -i -- 's,DUMMY,ljs_dataset_folder/wavs,g' filelists.txt
```

### 查看文件夹下的文件、数量

```bash
# 查看当前目录下的文件数量（不包括子文件）
ls -1 . | wc -l
# 查看当前目录下的所有文件数量（包括子文件）
find . -type f | wc -l

# 查看当前文件夹下名为‘.spec.pt’文件（包括子文件夹）
find . -type f -name "*.spec.pt"
# 查看当前文件夹下名为‘.spec.pt’文件数量（包括子文件夹）
find . -type f -name "*.spec.pt" | wc -l
```

### 批量删除'xxx.xx'文件

```bash
# 批量删除当前文件夹下名为‘.spec.pt’文件（包括子文件夹）
find . -type f -name "*.spec.pt" -exec rm -f {} +
```

## 服务器之间数据传输

**从本地上传到远程服务器：**

```python
#传输单个文件  “：冒号必须有”
scp /path/to/local/file.txt username@remote_host:/path/to/remote/directory/
#options:
#-r：递归到整个目录
#-v：显示详细操作过程
#传输文件夹
scp -r /path/to/local/file.txt username@remote_host:/path/to/remote/directory/
```

**从远程服务器复制到本地：**

```python
#传输单个文件“：冒号必须有”
scp username@remote_host:/path/to/remote/directory/ /path/to/local/file.txt
    
#传输文件夹
scp -r username@remote_host:/path/to/remote/directory/ /path/to/local/file.txt
```

## 后台训练（Screen）

**screen指令：**

```bash
# 创建新会话并保存日志：
screen -L -Logfile ~/xxx/xxx/log.txt  -S name
# 重新连接对话框：
screen -r name
# 临时退出：
ctrl+a → d
# 关闭清除对话框：
screen -S name -X quit
# 查看会话：
screen -ls
# 进入滚轮模式：
ctrl+a → [
# 退出滚轮模式：
Esc
```

## Mamba(平替conda）

使用conda时解析包与包之间的依赖花费时间太久，所以使用mamba代替conda可以快速安装并解析依赖文件。

- Mamba 支持所有常用 Conda 命令（`create`, `install`, `update`, `remove`, `list`, `search` 等），只需将 `conda` 替换为 `mamba`
- Mamba使用 Conda 的默认频道（`defaults`）、第三方频道（如 `conda-forge`, `pytorch`）和虚拟环境
- 可与 Conda 混用（如用 `conda` 创建环境，用 `mamba` 安装包）
- ==只需在base基础环境中安装一次即可==在其他虚拟环境下使用（`conda activate base  →→ conda instsall -c conda-forge mamba`）

```python
#使用过程：
#1.创建环境（使用conda创建）
conda create -n name python=3.xxx
#2.激活环境
conda activate name
#3.在虚拟环境下安装Mamba
conda install -c conda-forge mamba
#4.验证安装
mamba --version
```

## 环境搭建（pip and conda）

### 1.conda(mamba)和pip配合使用

含有requirements.txt文件时

- 首先使用conda安装：

  ```bash
  conda install --file requirements.txt -c conda-forge
  
  # 使用pip安装
  pip install -r requirements.txt -i 清华源
  ```

- 对 conda 安装不到的包用 pip 安装

  ```bash
  # 可加入镜像源（清华源、阿里源等）
  pip install -r requirements.txt
  ```

### 2.conda和mamba配合使用

已配好的环境，含有requirements.txt但不含有environment.yml

- 使用conda将配好的环境导出：

  ```bash
  conda env export > environment.yml
  
  # 使用pip将环境导出
  # 1.首先使用列出所有包
  pip list --format=freeze
  # 2.建立equirements.txt，将上述所有包复制进去
  ```

- 以后配环境就可以直接使用environment.yml：

  ```bash
  # 一键复现整个环境
  conda env create -f environment.yml
  # 加速
  mamba env create -f environment.yml
  ```

# 深度学习

## Pytorch库

### torch.arange()

```python 
# 随机生成特定个数的tensor张量

>>> torch.arange(5)
tensor([ 0,  1,  2,  3,  4])
# 左闭右开
>>> torch.arange(1, 4)
tensor([ 1,  2,  3])
# 生成1-2.5，每隔0.5生成一个
>>> torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])
```

### tensor.clamp()和tensor.clip()

```powershell
# 将tensor数据剪切到[start,end]区间，小于start的都赋值为start，大于end的都赋值为end

>>> a = torch.tensor([1, 2, 0.5, -0.5, -1.2])
>>> a
tensor([ 1.0000,  2.0000,  0.5000, -0.5000, -1.2000])
# 使用clamp
>>> b = a.clamp(-1,1)
>>> b
tensor([ 1.0000,  1.0000,  0.5000, -0.5000, -1.0000])
# 使用clip
>>> c = a.clip(-1,1)
>>> c
tensor([ 1.0000,  1.0000,  0.5000, -0.5000, -1.0000])
```

### torch.cat()

```python
# 在指定的维度上将两个或多个张量连接起来，生成一个新的张量。（拼接时张量形状必须相同）

# 生成a
>>> a = torch.randn(2,2)
>>> a
tensor([[ 0.5673, -0.5050],
        [-1.2899,  0.1413]])
# 生成b
>>> b = torch.zeros(2,2)
>>> b
tensor([[0., 0.],
        [0., 0.]])
# 生成c
>>> c = torch.ones_like(b)
>>> c
tensor([[1., 1.],
        [1., 1.]])
# 将a和b在第0维拼接，得到d
>>> d = torch.cat((a,b), dim=0)
>>> d
tensor([[ 0.5673, -0.5050],
        [-1.2899,  0.1413],
        [ 0.0000,  0.0000],
        [ 0.0000,  0.0000]])
>>> d.shape
torch.Size([4, 2])
# 将a和b在第1维拼接，得到e
>>> e = torch.cat((a,b), dim=1)
>>> e
tensor([[ 0.5673, -0.5050,  0.0000,  0.0000],
        [-1.2899,  0.1413,  0.0000,  0.0000]])
>>> e.shape
torch.Size([2, 4])
# 将e和c在第1维拼接，得到f
>>> f = torch.cat((e, c), dim=1)
>>> f
tensor([[ 0.5673, -0.5050,  0.0000,  0.0000,  1.0000,  1.0000],
        [-1.2899,  0.1413,  0.0000,  0.0000,  1.0000,  1.0000]])
>>> f.shape
torch.Size([2, 6])
```

### tensor.cpu()

```python 
# cpu() 方法用于将张量从 GPU 移动到 CPU
import torch

>>> device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
>>> device
device(type='cuda')
>>> a = torch.tensor([1, 2, 3])
# 这种赋值不会将a放在GPU上
>>> a.to(device)
tensor([1, 2, 3], device='cuda:0')
>>> a
tensor([1, 2, 3])
>>> a.device
device(type='cpu')
# 将a放到GPU上并'赋值'给b
>>> b=a.to(device)
>>> b
tensor([1, 2, 3], device='cuda:0')
>>> a
tensor([1, 2, 3])
# 这种方式也不能将b放在cpu上
>>> b.cpu()
tensor([1, 2, 3])
>>> b
tensor([1, 2, 3], device='cuda:0')
# 将b放到CPU上并'赋值'给c
>>> c=b.cpu()
>>> c
tensor([1, 2, 3])
>>> b
tensor([1, 2, 3], device='cuda:0')
```

### tensor.device和tensor.dtype

```python
# 查看数据在哪个设备上
# 查看数据类型
>>> a.device
device(type='cpu')
>>> b = a.to('cuda')
>>> b.device
device(type='cuda', index=0)

>>> c.dtype
torch.float32
```

### torch.detach()

```python
# detach() 方法用于返回一个新的张量，该张量与原张量共享相同的数据，但不再需要计算梯度。这通常用于在训练过程中# 分离出模型的前向传播结果，以防止在反向传播时计算梯度。这对于后续将张量转移 NumPy 很有用。
import torch

b = torch.tensor([1,2,3])
a = b.detach()
# a就是共享b数据，不需要计算梯度
```

### torch.eye()

```python
# 生成对角矩阵
>>> torch.eye(3)
tensor([[ 1.,  0.,  0.],
        [ 0.,  1.,  0.],
        [ 0.,  0.,  1.]])
```

### torch.linspace()

```python
# 在[start, end]之间生成 steps 个等间距的数
# 在[0,1]之间生成10个等间距的数，并将其放在GPU上
>>> c = torch.linspace(start=0,end=1,steps=10,device='cuda')
>>> c
tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889, 1.0000])
>>> c.shape
torch.Size([10])
```

### torch.mean()

```python
# 对张量的某一维度或者整个tensor进行求均值

import torch
>>> a=torch.randn(2,3)
>>> a
tensor([[ 0.3801,  0.6078, -1.8773],
        [-0.2862,  1.7689, -0.1001]])
>>> a.shape
torch.Size([2, 3])
# 对第0维求均值，并且保持tensor形状不变
>>> mean = torch.mean(a, dim=0, keepdim=True)
>>> mean.shape
torch.Size([1, 3])
# 对第0维求均值，tensor形状改变
>>> mean = torch.mean(a, dim=0)
>>> mean.shape
torch.Size([3])
```

### torch.numel()

```python
# 计算输入tensor的总数目
>>> a = torch.randn(1,2,3,4)
>>> print(torch.numel(a))
24
>>> a = torch.zeros(4,4)
>>> torch.numel(a)
16
```

### tensor.numpy()

```python
# numpy() 方法用于将 PyTorch 张量转换为 NumPy 数组。此操作只能应用于 CPU 上的张量

# 接上面程序
>>> b
tensor([1, 2, 3], device='cuda:0')
# c在CPU上所以可以成功转化成numpy
>>> c
tensor([1, 2, 3])
>>> d = c.numpy()
>>> d
array([1, 2, 3])

# 将b转化成numpy时会报错，因为b在GPU上
>>> b.numpy()
# Traceback (most recent call last):
#   File "<stdin>", line 1, in <module>
# TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor # to host memory first.
```

### tensor.permute()

```python
# 更改张量维度顺序

>>> import torch
# 原始维度（1,2,3）
>>> a = torch.randn(1, 2, 3)
>>> a.shape
torch.Size([1, 2, 3])
# 转变维度为（2,1,3），使用permute交换0,1维
>>> convert_a = a.permute(1, 0, 2)
>>> convert_a.shape
torch.Size([2, 1, 3])
```

### torch.nn.functional.pad()

```python
# 模式默认为mode='constant'，还有其他模式'reflect'等

import torch
import torch.nn.functional as F

input = torch.tensor([[1, 2], [3, 4]])
# padding中的1,1指倒数第0维左右两侧填充1个value，
# 	   	    2,2指倒数第1维左右两侧填充2个value
padding = (1, 1, 2, 2)
padded = F.pad(input, padding, value=0)
print(padded)

#结果
tensor([[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 1, 2, 0],
        [0, 3, 4, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]])
# 在最后一维（宽度）左右填充了1个0
# 在倒数第二维（高度）上下各填充了2个0
```

### tensor.reshape()

```python
# 改变张量的形状（和torch.view()用法一致）
'''
主要区别
1.内存共享：
view 函数要求新的形状与原始张量共享相同的内存，意味着它只能在张量是连续的情况下使用。
reshape 会根据需要返回一个新张量，因此它不一定要求内存的连续性。

2.连续性检查：
使用 view 时，如果张量不是连续的会抛出错误，必须使用 tensor.contiguous() 函数来确保张量是连续的后才能调用 view。
reshape 如果检测到张量不连续，它会自动返回该张量的一个拷贝。

3.返回结果：
view 返回的是一个与原始张量共享相同内存的视图。
reshape 在可能的情况下返回视图，但如果需要返回新的拷贝，它也会做到。
'''

>>> import torch
>>> a = torch.arange(10)
>>> a
tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> a.shape
torch.Size([10])
# 1.改变原来张量的形状（必须能被整除比如2*5，不能是2*4！！）
>>> b = a.reshape(2,5)
>>> b.shape
torch.Size([2, 5])
# （必须能被整除比如2*5，不能是2*4！！）
>>> b = a.reshape(2,4)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: shape '[2, 4]' is invalid for input of size 10
# 2.给入-1自动匹配
>>> b = a.reshape(5,-1)
>>> b.shape
torch.Size([5, 2])
>>> c = b.reshape(-1)
>>> a.shape
torch.Size([10])
```

### torch.rand()

```python
# 生成的随机数随机均匀分布在 [0,1)，适合做均匀随机基础数据。

>>> a = torch.rand(5,4)
>>> a
tensor([[0.3376, 0.5445, 0.3784, 0.4731],
        [0.4329, 0.9294, 0.6844, 0.3519],
        [0.5503, 0.5150, 0.4348, 0.7986],
        [0.5190, 0.9588, 0.9660, 0.8582],
        [0.8868, 0.9204, 0.0064, 0.1283]])
```

### torch.randn()

```python
# 生成一个形状指定的张量，元素服从 标准正态分布（高斯分布），均值为 0，标准差为 1。
# 更适合模拟自然界噪声或正态分布随机变量。

>>> a=torch.randn(5,4)
>>> a
tensor([[ 0.6011, -0.5739,  0.3006,  0.2820],
        [ 1.2011,  1.4002, -0.4396, -1.2930],
        [ 0.1473,  0.9390, -0.6608, -1.4537],
        [ 1.6214, -1.5094,  0.0848,  0.2891],
        [ 0.9942,  2.3460,  0.8703, -0.2629]])
```

### torch.std()

```python
# 对张量的某一维度或整个tensor进行求标准差

import torch
>>> a=torch.randn(2,3)
>>> a
tensor([[ 0.3801,  0.6078, -1.8773],
        [-0.2862,  1.7689, -0.1001]])
>>> a.shape
torch.Size([2, 3])
# 对第0维求均值，并且保持tensor形状不变
>>> std = torch.std(a, dim=0, keepdim=True)
>>> std.shape
torch.Size([1, 3])
# 对第0维求均值，tensor形状改变
>>> std = torch.std(a, dim=0)
>>> std.shape
torch.Size([3])
```

### torch.stack()

```python
# 将一系列张量沿着新的维度（轴）拼接（堆叠）起来的函数。和 torch.cat 有点类似，但 stack 会在指定的位置增加一个新的维度。

# 将多个维度相同的张量沿着一个新的维度（dim）堆叠起来，结果的维度比输入张量多1。
>>> a = torch.tensor([1, 2, 3])
>>> b = torch.tensor([4, 5, 6])
>>> c = torch.tensor([7, 8, 9])
>>> stacked0 = torch.stack([a, b, c], dim=0)	# 在第0维堆叠
>>> stacked0
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
>>> stacked0.shape
torch.Size([3, 3])
>>> stacked1 = torch.stack([a, b, c], dim=1)	# 在第1维堆叠
>>> stacked1
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
>>> stacked1.shape
torch.Size([3, 3])
```

### torch.squeeze()

```python
# 对tensor数据处理时，用于移除张量中所有大小为 1 的维度。比如，如果 waveform 的形状是 (1, 10)，使用 
# squeeze() 后，结果的形状将变为 (10,)。如果没有维度为 1，则返回的张量与输入张量相同。
import torch

>>> a = torch.randn(1, 1, 4, 2, 1, 2)
>>> a.shape
torch.Size([1, 1, 4, 2, 1, 2])
# 使用squeeze()去除维度为1的维度
>>> b = a.squeeze()
>>> b.shape
torch.Size([4, 2, 2])
```

### torch.tensor()

```python
# 生成tensor张量
torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])

torch.tensor([0, 1])  # Type inference on data

torch.tensor([[0.11111, 0.222222, 0.3333333]],
             dtype=torch.float64,
             device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device

torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor

torch.tensor([])  # Create an empty tensor (of size (0,))
```

### torch.unsqueeze()

```python
# 指定位置插入一个维度为 1 的新张量。

import torch
>>> x = torch.tensor([1, 2, 3, 4])
>>> x.shape
torch.Size([4])

# 在第0维添加新张量
>>> x1 = torch.unsqueeze(x, 0)
>>> x1.shape
torch.Size([1, 4])
>>> x1
tensor([[1, 2, 3, 4]])
# 在第1维添加新张量
>>> x2 = torch.unsqueeze(x, 1)
>>> x2.shape
torch.Size([4, 1])
>>> x2
tensor([[1],
        [2],
        [3],
        [4]])
```

### tensor.view()

```python
# 改变张量的形状（和torch.reshape()用法一致）
'''
主要区别
1.内存共享：
view 函数要求新的形状与原始张量共享相同的内存，意味着它只能在张量是连续的情况下使用。
reshape 会根据需要返回一个新张量，因此它不一定要求内存的连续性。

2.连续性检查：
使用 view 时，如果张量不是连续的会抛出错误，必须使用 tensor.contiguous() 函数来确保张量是连续的后才能调用 view。
reshape 如果检测到张量不连续，它会自动返回该张量的一个拷贝。

3.返回结果：
view 返回的是一个与原始张量共享相同内存的视图。
reshape 在可能的情况下返回视图，但如果需要返回新的拷贝，它也会做到。
'''

# 1.在Pytorch中一般使用view(-1)将多维张量转化为一维张量（只改变张量的形状不改变数值）
a=torch.randn(4, 4)
>>> a.size()
torch.Size([4, 4])
>>> b=a.view(-1)
>>> b.size()
torch.Size([16])
>>> b
tensor([-0.7595,  0.6440, -0.1086,  1.0570, -0.0189,  0.0699, -0.7900, -1.0413,
         0.7684, -1.1192,  0.4579,  1.1159,  1.6170,  0.0934, -0.4992, -0.2790])
>>> a
tensor([[-0.7595,  0.6440, -0.1086,  1.0570],
        [-0.0189,  0.0699, -0.7900, -1.0413],
        [ 0.7684, -1.1192,  0.4579,  1.1159],
        [ 1.6170,  0.0934, -0.4992, -0.2790]])


# 2.改变torch的形状，比如某一维是4可拆分为（2*2）！！！（某维是3不可以拆成1*2！！！）
>>> a = torch.randn(1,2,4)
>>> a.shape
torch.Size([1, 2, 4])
>>> d = a.shape[:-1]# 除最后一维其他维度都保留
>>> a = a.view(*d, 2,2)# 将4拆成2*2
>>> a.shape
torch.Size([1, 2, 2, 2])
```

## Lhotse库（构建和管理语音数据集）

### Mainfest（元数据）

生成数据集的清单。

**指令：**

- `lhotse prepare libritts`

  调用 Lhotse 的命令行接口（CLI），执行“prepare”命令，准备 `libritts` 这个数据集。

  Lhotse 是一个用于语音数据集准备和管理的库，支持很多公开语音数据集的标准化和快速准备。

- `--num-jobs ${nj}`

  指定并行作业数量（`nj` 是变量，通常代表“number of jobs”），表示命令会使用多少并行线程/进程来加速数据准备。

- `$dl_dir/LibriTTS`

  指向下载好的 LibriTTS 数据集根目录，Lhotse 会基于这个路径读取原始音频文件和标注文件。

- `data/manifests`

  指定生成的清单（manifests）文件输出目录。Lhotse 会把音频切片、长度、转录等信息以结构化 JSON 或其他格式，写入到这里，方便后续训练流程使用。

```bash
lhotse prepare libritts --num-jobs ${nj} $dl_dir/LibriTTS data/manifests
```

#### Recording

`Recording` 表示一个音频文件及其相关信息。它通常包含以下信息：

- **音频路径**：音频文件的存储位置。
- **时长**：音频的总时长。
- **采样率**：音频文件的采样率（如 16kHz，44.1kHz 等）。
- **音频格式**：音频文件的格式，如 WAV、MP3 等。

```json
{
  "id": "recording-001",
  "sources": [{"type": "file", 
               "channels": [0], 
               "source": "libritts/001.wav"}],
  "duration": 10.5,
  "sampling_rate": 16000，
  "num_samples": 34081, 
  "duration": 1.4200416666666666, 
  "channel_ids": [0]
}
```

#### Supervision

`Supervision` 表示与特定音频文件或音频片段相关联的标签或标注信息。这通常包括：

- **转录文本**：音频中的文本内容，通常是语音识别的目标输出。
- **对齐信息**（可选）：如果你有对齐信息（如时间戳），`Supervision` 也可以存储这些信息。
- **说话人信息**（可选）：例如音频文件所属的说话人。
- **语言**（可选）：该音频片段的语言信息。

```json
{
  "id": "supervision-001",
  "recording_id": "recording-001",
  "start": 2.0,
  "duration": 3.5,
  "channel": 0,
  # 因为原文本中有'符号，所以必须使用 \ 转义
  "text": "\"He isn't fit to hear what's said here.",
  "language": "English",
  "speaker": "speaker1",
  "custom": {"orig_text": "\"He isn't fit to hear what's said here.", 
             "snr": 21.033476}
}

```

## OS库

### os.path.basename()

```python
# 给定一个路径字符串，返回该路径的最后一级名称，通常是文件名（或者目录名，如果路径以目录结尾）。

import os

path1 = "/home/user/documents/report.pdf"
filename1 = os.path.basename(path1)
print(filename1)  # 输出: report.pdf
```

### os.walk()

```python
# 递归目录遍历工具，会生成一个三元组 (dirpath, dirnames, filenames)，便于遍历整个目录树。


```

### os.listdir()

```python
# 列出当前目录下所有的file和dir


```

### os.path.join()



### os.makedirs()

```
 
```

### os.path.exists()

### os.path.getsize()

## Path库

1. **创建文件夹和文件：**

   导入Path库，创建Path对象使用Path包方便处理文件

   ```python
   # 导入Path包
   from pathlib import Path
   # 创建目标文件（这里可能存在，也可能不存在）
   file = "/home/hhc/project/voice_recognition/mmmm/model.txt"
   # 创建Path对象
   path = Path(file)
   # 判断文件的父类文件夹是否存在，parents=True不存在则创建，exist_ok=True存在则不报错
   path.parent.mkdir(parents=True, exist_ok=True)
   # 以上代码创建父类目录，但不会创建model.txt
   # 使用open即可自动建立model.txt
   with open(path, 'w') as f:
       f.write("Hello")
   ```

2. **检查文件类型：**

   ```python
   from pathlib import Path
   
   # 必须创建Path对象
   file_path = Path("file_name_xxx")
   dir_path = Path("file_name_xxx")
   print(file_path.is_file())  # 检查是否是文件
   print(dir_path.is_dir())     # 检查是否是目录
   ```

3. **遍历目录：**

   ```python
   from pathlib import Path
   
   # 1.glob使用
   # 创建Path对象
   dir_path=Path("file_name_xxx")
   # 查找所有文件，当dir_path是目录时
   for file in dir_path.glob("*"):
       print(file)
   # 只打印该路径下的文件名，不会递归查看子文件夹下的文件
   # dir_path/
   #    ├── speakers/
   #    ├── music/
   #    ├── images/
   #    └── notes.txt
   
   # 2.rglob使用（常用！！！！）
   # 创建Path对象
   dir_path=Path("file_name_xxx")
   list_path=[]
   # 查找该目录下的所有.wav文件并输出路径名称（包括子文件夹下的.wav文件）
   for file in dir_path.rglob("*.wav"):
       print(file)
       list_path.append(file)
   # /home/hhc/project/video1/1/2134.5_2135.3.wav
   # /home/hhc/project/video1/1/151.4_151.8.wav
   # /home/hhc/project/video1/1/1717.5_1719.5.wav
   # ......
   ```

4. **将遍历的文件路径名称写入.txt中**

   ```python
   from pathlib import Path
   
   # 遍历文件目录
   dir_path='/home/hhc/project/video1/'
   # 创建Path对象
   dir_path = Path(dir_path)
   # 保存所有的.wav文件
   list_path=[]
   for file in dir_path.rglob("*.wav"):
       list_path.append(file)
   
   # 创建写入文件.txt（ / 被用作连接符 ）
   file_path = dir_path / 'file_wav.txt'
   
   '''
   'a'（append）：以追加模式打开文件。如果文件不存在，将会创建一个新文件；如果文件存在，新的内容将被写入到	文件的末尾，而不覆盖原有的内容。
   'r'：以读取模式打开文件（默认模式）。文件必须已经存在。
   'w'：以写入模式打开文件。如果文件存在，内容将被清空；如果文件不存在，将会创建一个新文件。
   'x'：以独占写入模式打开文件。文件必须不存在，否则会抛出异常。
   'b'：以二进制模式打开文件(可以与其他模式结合使用)。例如，'rb'表示以二进制读取模式打开。
   't'：以文本模式打开文件（默认模式）。例如，'rt'表示以文本读取模式打开。
   'a+'：以追加和读取模式打开文件，允许在文件末尾追加内容，同时也能够读取文件内容。
   '''
   
   # 这里'w'表示写入，但是如果追加内容的话建议使用'a'
   with open(file_path, 'w') as f:
       for file in list_path:
           # 将读取出的file转化成str类型最后加上\n（file是PosixPath类型）
           f.write(f'{file}\n')
           
   ```

## yaml

### !new、!name、!ref 使用

```python
# 1. !new是实例化类，并初始化将参数传入
llm: !new:cosyvoice.llm.llm.Qwen2LM
    # !ref指引用上述定义的全局参数
    llm_input_size: !ref <llm_input_size>
    llm_output_size: !ref <llm_output_size>
    speech_token_size: 6561
    length_normalized_loss: True
    lsm_weight: 0
    mix_ratio: [5, 15]
# 相当于
model = Qwen2LM(
    llm_input_size=...,
    llm_output_size=...,
    speech_token_size=6561,
    length_normalized_loss=True,
    lsm_weight=0,
    mix_ratio=[5, 15]
）

# 2. !name指生成一个函数实例，并将下面参数传入函数内
sampling: !name:cosyvoice.utils.common.ras_sampling
    top_p: 0.8
    top_k: 25
    win_size: 10
    tau_r: 0.1
# 相当于
from cosyvoice.utils.common import ras_sampling
sampling = ras_sampling(top_p=0.8, top_k=25, win_size=10, tau_r=0.1)
    
    
# 3.综合（类中也可以传入函数和类）
llm: !new:cosyvoice.llm.llm.Qwen2LM
    llm_input_size: !ref <llm_input_size>
    llm_output_size: !ref <llm_output_size>
    speech_token_size: 6561
    length_normalized_loss: True
    lsm_weight: 0
    mix_ratio: [5, 15]
    llm: !new:cosyvoice.llm.llm.Qwen2Encoder
        pretrain_path: !ref <qwen_pretrain_path>
    sampling: !name:cosyvoice.utils.common.ras_sampling
        top_p: 0.8
        top_k: 25
        win_size: 10
        tau_r: 0.1
# 相当于
from cosyvoice.llm.llm import Qwen2LM

model = Qwen2LM(
    llm_input_size=...,
    llm_output_size=...,
    speech_token_size=6561,
    length_normalized_loss=True,
    lsm_weight=0,
    mix_ratio=[5, 15],
    llm=Qwen2Encoder(pretrain_path=...),
    sampling=ras_sampling(top_p=0.8, top_k=25, win_size=10, tau_r=0.1)
)
```

## Tensorboard

`将 PyTorch 的训练过程与 TensorBoard 结合起来，使得用户可以方便地可视化训练过程中的各种指标、模型图和其他有用的信息，PyTorch 1.1 版本及以上已经包含了 torch.utils.tensorboard，所以只需要使用pip安装tensorboard即可`

Tensorboard使用详情 官网地址：*[torch.utils.tensorboard — PyTorch 2.7 documentation](https://docs.pytorch.org/docs/stable/tensorboard.html)*

**使用过程：**

1. **安装：**

   ```python
   pip install tensorboard
   ```

2. **使用：**

   ```python
   # 导入库
   from torch.utils.tensorboard import SummaryWriter
   # 使用SummaryWriter创建logs
   writer = SummaryWriter(f'logs/{model_tag}')
   # 向其中添加数据
   writer.add_scalar('train_accuracy', train_accuracy, epoch)
   writer.add_scalar('val_accuracy', val_accuracy, epoch)
   writer.add_scalar('training_loss', running_loss, epoch)
   writer.add_scalar('val_EER', val_err, epoch)
   # 最后使用在终端运行
   tensorboard --logdir=logs（不需要加其他路径）
   # 然后会生成一个网址，直接打开网址即可（训练中运行指令和训练完成后运行指令都可以！）
   ```
   
3. **注意：**

   如果运行tensorboard --logdir=logs时发生错误，有可能是`protobuf` 和 `tensorboard` 的版本不兼容的问题。

   可以**降级protobuf**的版本或者**升级tensorboard**的版本。

   ![1753332591361](深度学习.assets/1753332591361.png)

## Pytorch-Summary

Pytorch-Summary是一个api可供下载，如想使用查看模型的各层、输出大小、每层的参数大小等模型具体信息，可以使用pip进行下载，使用过程如下。

**github官网地址：** *[Pytorch-Summary](https://github.com/sksq96/pytorch-summary)*

1. **安装：**在指定的虚拟环境下：

   ```python
   pip install torchsummary
   ```

2. **导入库使用：**

   ```python
   # 导入库
   from torchsummary import summary
   # 将模型实例化
   model = mymodel()
   # model：实例化的模型；input_size:单个样本的形状；batch_size：一个批次的大小;device：设备名称
   summary(model, input_size=(channels, H, W),batch_size=128,device)
   # 注意：如果每个输入数据大小是1维的则input_size=（64000，）!!!不能使用常数int
   #       必须将数据和模型放在同一device上（cpu或者gpu，一般选择cpu方便测试）
   # 例如：查看train set的DataLoader数据大小是torch.size（[128, 64000]）,128:batch_size;64600:数据大小
   if __name__ == '__main__':
       device = "cpu" 
       model = RawNet2(device)
       summary(model, (64000,), batch_size=128, device=device)
   ```



# 基础知识

## 模型（Model）

### 使用FLOPs计算模型的复杂度

------

**定义：**
FLOPs指的是每秒执行的浮点计算次数，而不是针对特定的时间单位。例如，一个算法的FLOPs可能是500 MFLOPs（百万浮点运算每秒），表示它在一秒内可以执行500万次浮点计算。

**计算范围：**
`全连接层、卷积层、激活函数、池化层`

**代码：**
使用thop包里的profile函数，在输入特征的情况下计算模型的参数和FLOPs。

```python
    from thop import profile
    x_np = torch.randn(1, 200, 80)
    flops, params = profile(model, inputs=(x_np, ))
    print("FLOPs: {} G, Params: {} M".format(flops / 1e9, params / 1e6))
```

### 模型结构和各层参数

------

- 查看模型结构：

```python
model = ResNet34(feat_dim=80, embed_dim=256, two_emb_layer=False)
print(model)
```

- 查看模型的各层参数和张量：

  - model.parameters（）的含义和代码：
  - **用 model.parameters()**：当只需要参数张量（如优化器传参）

  ![1752905892945](../Typora%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/1752905892945.png)

  ```python
  import torch.nn as nn
  model = nn.Linear(10, 2)
  for param in model.parameters():
      print(param.shape)  # 输出：torch.Size([2, 10]) 和 torch.Size([2])
  ```

  - model.named_parameters()的含义和代码：
  - **用 model.named_parameters()**：当需要参数名称或精细控制参数（如调试、冻结层、初始化）

  ![1752906026347](../Typora%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/1752906026347.png)

```python
for name, param in model.named_parameters():
    print(f"名称: {name}, 形状: {param.shape}, 是否需要梯度: {param.requires_grad}")
```

### 参数量计算

```python
# 计算模型的总参数量
total_params = sum(p.numel() for p in model.parameters())
    print(f"model模型总参数数量: {total_params/1e6}")
    
# 查看参数的数据类型（打印出每一层的名字和参数类型）FP32 (float32)、FP16 (float16)、BF16 (bfloat16)、INT8
for name, param in model.named_parameters():
    print(name, param.dtype)
```

## Pytorch下载（pip and Wheel）

Pytorch安装时有两个版本：**Wheel和Conda**

- Wheel：使用pip安装，速度快但是有可能出现不兼容问题。
- Conda：使用Conda安装，速度慢但是可以避免环境不兼容问题。

## 什么是梯度累加？

**梯度累加（Gradient Accumulation）** 是一种在深度学习训练中常用的技术，用来**在不增加显存（GPU 内存）消耗的情况下，实现更大的“等效批量大小（effective batch size）”**。

梯度累加的核心思想是：

> 不每次计算完一个小批量（mini-batch）就更新参数，而是**累计多个小批量的梯度**，然后再**统一更新一次参数**。这样就可以在**有限的显存**上实现对大批次（多个小批次）的数据计算梯度，使用**大 batch size** 训练模型可以带来更稳定的梯度估计，甚至更好的收敛性能。

### 单机单卡使用梯度累加

```python
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, TensorDataset

# ==== 模拟数据 ====
x = torch.randn(1000, 10)
y = torch.randn(1000, 1)
dataset = TensorDataset(x, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# ==== 模型与优化器 ====
model = nn.Linear(10, 1).cuda()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# ==== 梯度累加参数 ====
accum_steps = 4  # 累加4次梯度，相当于 batch_size=32*4=128

# ==== 训练循环 ====
for epoch in range(5):
    optimizer.zero_grad()
    for step, (inputs, targets) in enumerate(dataloader):
        inputs, targets = inputs.cuda(), targets.cuda()

        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss = loss / accum_steps     # 平均 loss，避免梯度放大
        loss.backward()               # 累积梯度

        # 每accum_steps步更新一次参数
        if (step + 1) % accum_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

    print(f"Epoch {epoch+1} done.")

```

### 单机多卡使用梯度累加

```python
# 关键技巧是使用：with model.no_sync():

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset
from torch import nn, optim
import os

def setup():
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))

def cleanup():
    dist.destroy_process_group()

def main():
    setup()

    # ==== 模拟数据 ====
    x = torch.randn(1000, 10)
    y = torch.randn(1000, 1)
    dataset = TensorDataset(x, y)
    sampler = DistributedSampler(dataset)
    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)

    # ==== 模型 ====
    model = nn.Linear(10, 1).cuda()
    model = DDP(model, device_ids=[int(os.environ["LOCAL_RANK"])])
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    accum_steps = 4

    # ==== 训练 ====
    for epoch in range(5):
        sampler.set_epoch(epoch)
        optimizer.zero_grad()

        for step, (inputs, targets) in enumerate(dataloader):
            inputs, targets = inputs.cuda(), targets.cuda()

            # 非累积步禁止同步梯度
            if (step + 1) % accum_steps != 0:
                with model.no_sync():
                    loss = criterion(model(inputs), targets)
                    (loss / accum_steps).backward()
            else:
                loss = criterion(model(inputs), targets)
                (loss / accum_steps).backward()
                optimizer.step()
                optimizer.zero_grad()

        if dist.get_rank() == 0:
            print(f"Epoch {epoch+1} done.")

    cleanup()

if __name__ == "__main__":
    main()

```

## 什么是梯度缩放因子？

在深度学习训练过程中，模型参数的更新依赖于反向传播得到的梯度。混合精度训练中，一部分计算用半精度浮点数（FP16）完成，而FP16相比传统的单精度浮点数（FP32），有更小的表示范围和精度，尤其是它能表示的最小数值要大得多。

**问题**：FP16的动态范围窄，容易导致梯度出现“下溢”（即梯度数值变得非常小，接近零）或者精度损失，最终导致梯度消失，训练失败。

### 梯度缩放因子的作用

**梯度缩放因子**就是一个放大系数，用来在计算梯度前先把梯度放大若干倍，保证梯度的数值远离FP16的下溢范围，提高数值精度。

具体流程是：

1. **前向和反向计算**用FP16执行，提升速度和降低显存占用。
2. **反向计算得到的梯度乘以梯度缩放因子**，放大梯度数值。
3. 使用放大后的梯度更新模型参数（仍然可能转换回FP32进行累积或更新）。
4. 之后，**将缩放后的梯度除以相同的缩放因子**，保证数值相等。

通过这种方法，避免了因为FP16太小导致梯度消失，保证训练稳定。

### 动态调整梯度缩放因子

因为不同模型、不同训练阶段对梯度大小的要求不同，直接用固定的缩放因子可能效果不好。通常使用**动态梯度缩放**：

- 当检测到梯度溢出（数值爆炸）时，减小缩放因子，防止数值无限大。
- 当训练稳定时，逐渐增大缩放因子，让训练更精细。
- 这样，训练过程中会自动调整缩放因子的大小，尽量保持数值在合理范围内。

PyTorch中的`torch.cuda.amp.GradScaler`就是实现这种动态缩放的工具。

### 简单类比

想象用放大镜看非常微小的字（梯度）：

- 没有梯度缩放因子：字体太小（FP16精度不够），根本看不清，训练出错。
- 有了梯度缩放因子：用放大镜把字放大，确保看清楚，再微调（更新模型）。

## Sampler和Collate

```markdown
Dataset.__init__()
       ↓
Sampler / BatchSampler  → 决定“取哪些 index”
       ↓
Dataset.__getitem__()
       ↓
collate_fn               → 把这些 index 对应的样本合并成一个 batch
       ↓
返回给训练循环
```

### Sampler

包括**batch_sampler**和**Sampler**

**Sampler：**控制样本的顺序

**Batch_sampler：**同时控制**采样顺序**和**批大小**

- 决定每一批中**有哪些样本 index**
- `DataLoader` 在每次迭代时，就会调用 `next(batch_sampler_iter)` 来拿到这些 index。

- `batch_sampler` 是一个 **可迭代对象**，每次 `__iter__` 返回一个 `batch`（一组样本的 index）。

```python
# 索引号
[0, 1, 2, 3]
[4, 5, 6, 7]
...
```

##### 固定帧数 Sampler（F5-TTS）

**思路**：

> - 不关心样本数量，只看 batch 的总帧数（`Σ frame_len ≤ max_frames`）。
> - 短样本 → batch 可以装更多个样本；
>    长样本 → batch 只能装少量样本。
> - 这样每个 batch 的“显存负载”大致恒定。

##### 分桶 Sampler（Vits）

**思路**：

> - 先对数据按长度排序，再分成多个“桶”（bucket）。
> - 每个 bucket 的样本长度接近。
> - 每次从同一个 bucket 中随机取固定数量的样本（例如 batch_size=16）。

**对比：**

| 指标         | 固定帧数Sampler | 分桶Sampler | 结合方式（推荐） |
| ------------ | --------------- | ----------- | ---------------- |
| 显存波动     | ✅ 稳定          | ⚠️ 波动      | ✅ 稳定           |
| Padding 浪费 | ⚠️ 中等          | ✅ 最小      | ✅ 较小           |
| GPU 利用率   | ✅ 高            | ⚠️ 不稳      | ✅ 高             |
| 实现复杂度   | ⚠️ 较高          | ✅ 简单      | ⚙️ 中等           |
| 最适用任务   | TTS / Diffusion | ASR / NLP   | TTS / ASR 大模型 |

### Collate

把这些 index 对应的样本合并成一个可以喂给模型的 batch（如 padding、stack、转换为 tensor）

当 `DataLoader` 拿到一批 index（比如 `[0, 1, 2, 3]`）后，会依次调用：

```python
# 将Sampler得到的索引号取出来，并调用Dataset取出数据拼成一个batch
samples = [dataset[i] for i in batch_indices]
batch = collate_fn(samples)
```

`collate_fn` 的**输入**是：

- 一个 list，长度 = batch_size
- 每个元素是 `Dataset.__getitem__` 的返回值

`collate_fn` 的**输出**是：

- 可以直接喂进模型的 batch，例如：
  - 张量（Tensor）
  - 字典（Dict[str, Tensor]）
  - 元组（Tuple）

## KL散度

KL 散度（Kullback-Leibler Divergence）精确地衡量了当我们使用一个**近似概率分布 Q** 来建模或描述一个**真实概率分布 P** 时，所引入的**信息损失**。

简而言之，它量化了**“近似”与“真实”之间的差距**。KL 散度值越小，意味着分布 Q 对分布 P 的拟合程度越高。

## 预训练模型下载

国内一般情况下使用huggingface下载模型时都会显示时间超时，所以一般都在`modelscope`网站上下载模型。在我们组服务器上有时候在modelscope也下载不了，一般采用在自己电脑上下载后，再上传到服务器上。步骤如下：

1. 首先进入模型网站：[模型库首页 · 魔搭社区](https://www.modelscope.cn/models)

2. 搜索想要下载的预训练模型

   ![1753794085560](../Typora%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/1753794085560.png)

3. 在Anaconda Prompt终端打开Conda虚拟环境，在指定环境安装modelscope包（pip install modelscope），安装完毕后在命令行输入以下代码：

   ```bash
   modelscope download --model AI-ModelScope/wav2vec2-base-960h
   # AI-ModelScope/wav2vec2-base-960h路径如上图所示
   ```

   下载后文件一般存储在：`C:\Users\<你的用户名>\.cache\modelscope\hub`

4. 就可以将上述文件夹下的模型上传至服务器使用。

# 语音伪造检测

## 思路

**发现**：单纯的SSL+后端分类网络在对抗攻击（白盒攻击PGD、FGSM）下EER直线上升（就是检测性能下降）；

![1764147889412](深度学习.assets/1764147889412.png)

**设计目标模型**：设计模型可以抵抗这种攻击（在白盒攻击下（攻击原来的SSL模型的数据）性能不受影响）

- 分为三个阶段训练：
  - **第一阶段：**在SSL网络中插入**伪造专家**，并适用LoRA训练伪造专家，冻结**ResNet**和 **SSL原有模型**
    - 训练集使用 **ASVspoof5train、ASVspoof5dev 和 ASVspoof2019train**；
    - 验证集使用 **ASVspoof2019dev**
  - **第二阶段：**训练ResNet18网络——检测**clean**（真实和伪造）和 **adv_data**（分类clean和adv）
    - 训练数据集使用 **ASVspoof2019train 、ASVspoof2019train_adv、ASVspoof5train 和 ASVspoof5train_adv**
  - **第三阶段：**

**文件：**

- **spoof_ssl：**单纯的SSL模型+分类（用来做对照实验的）
  - 包含生成对抗数据的脚本1.py、2.py—— **需要生成：ASVspoof5train_adv（PGD）、ASVspoof2019train_adv（PGD） 和 ASVspoof2019eval_adv（FGSM）**

## 实验

### ASVSpoof5 - baseline（训练集：24train）

|  模型   | 时长（s） | 损失函数 | 测试集 |  EER  |
| :-----: | :-------: | :------: | :----: | :---: |
| AASIST  |     4     | CE-loss  | 24eval | 29.12 |
| RawNet2 |     4     | CE-loss  | 24eval | 36.04 |

### 各种基线（训练集：LA19train）

|       模型       |          测试集          |            EER            |
| :--------------: | :----------------------: | :-----------------------: |
|  AASIST^[1][6]^  | LA19/ LA21/ DF21/ 24eval | 0.83/ 11.46/ 21.06/ 35.63 |
| RawNet2^[4][5]^  | LA19/ LA21/ DF21/ 24eval |  1.14/ 9.5/ 22.38/ 37.52  |
|     SLS^[2]^     | LA19/ LA21/ DF21/ 24eval |  0.23/ 1.92/ 2.87/ 18.76  |
| WavLM-large^[3]^ | LA19/ LA21/ DF21/ 24eval |  0.38/ 6.97/ 6.24/ 16.39  |

|    模型     |          测试集          |            EER            |
| :---------: | :----------------------: | :-----------------------: |
|   AASIST    | LA19/ LA21/ DF21/ 24eval | 0.83/ 11.46/ 21.06/ 35.63 |
|   RawNet2   | LA19/ LA21/ DF21/ 24eval |  1.14/ 9.5/ 22.38/ 37.52  |
|     SLS     | LA19/ LA21/ DF21/ 24eval |  0.23/ 1.92/ 2.87/ 18.76  |
| WavLM-large | LA19/ LA21/ DF21/ 24eval |  0.38/ 6.97/ 6.24/ 16.39  |

[1] Exploring generalization to unseen audio data for spoofing: Insights from ssl models
[2] Audio Deepfake Detection with Self-Supervised XLS-R and SLS Classifier
[3] Comprehensive Layer-wise Analysis of SSL Models for Audio Deepfake Detection
[4] Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection.
[5] End-to-end anti-spoofing with RawNet2
[6] AASIST: Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks
[]

### Whisper（训练集：LA19train）

|      模型      | 时长（s） | 层数 | 损失函数 | 测试集 |   EER   |
| :------------: | :-------: | :--: | :------: | :----: | :-----: |
|  whisper_tiny  |    30     |  4   | CE-loss  |  19LA  | 11.5961 |
|  whisper_base  |    30     |  6   | CE-loss  |  19LA  | 9.9525  |
| whisper_small  |    30     |  12  | CE-loss  |  19LA  | 0.7488  |
| whisper_medium |    30     |  24  | CE-loss  |  19LA  | 0.5151  |

### SSL（训练集：LA19train）

|       模型       | 时长（s） | 层数 | 损失函数  |   测试集    |         EER         |
| :--------------: | :-------: | :--: | :-------: | :---------: | :-----------------: |
|    WavLM_Base    |     4     |  12  |  CE-loss  | 19LA/21LA/5 | 0.7179/13.3157/19.7 |
| WavLM_Base(加噪) |     4     |  12  |  CE-loss  | 19LA/21LA/5 | 1.5636/10.7305/20.8 |
|    WavLM_Base    |     4     |  12  | ohem-loss | 19LA/21LA/5 | 1.1285/12.7194/21.1 |
| WavLM_Base(加噪) |     4     |  12  | ohem-loss | 19LA/21LA/5 | 1.1016/7.3552/19.4  |

### SSL（训练集：19LAtrain；测试集：19LAeval；loss：CE-loss）——对抗攻击

|       模型       |   攻击方式   | 时长/层数 | EER（攻击前后） |
| :--------------: | :----------: | :-------: | :-------------: |
|    WavLM_Base    | 白盒（PGD）  |   4/12    |  0.830/99.986   |
|    WavLM_Base    | 白盒（FGSM） |   4/12    |  0.830/21.906   |
| WavLM_Base(加噪) | 白盒（PGD）  |   4/12    |  1.578/99.986   |
| WavLM_Base(加噪) | 白盒（FGSM） |   4/12    |  1.578/10.195   |

### 融合（训练集19LAtrain）

|                    特征                    |            模型             |     损失函数      |  测试集   |  时长（s）  |       EER        |
| :----------------------------------------: | :-------------------------: | :---------------: | :-------: | :---------: | :--------------: |
|                    CQT                     |          ResNet18           |      CE-loss      |   19LA    |             |     3.99763      |
|                    CQT                     |          ResNet18           |     OHEM-loss     |   19LA    |             |     4.02454      |
|               SSL（不加噪）                |         WavLM-large         |      CE-loss      |  19LA/5   |      4      | 0.47975/18.32959 |
|                SSL（加噪）                 |         WavLM-large         |      CE-loss      | 19LA/21LA |      4      |     0.568855     |
| Whisper embedding和SSL embedding（未加噪） |  WavLM-base和Whisper-base   | CE-loss和对比损失 |   19LA    | 30+30、30+4 |  0.8156、1.3322  |
|  Whisper embedding和SSL embedding（加噪）  | WavLM-large和Whisper-medium | CE-loss和对比损失 |   19LA    |     30      |     0.367481     |

### 数据集质量对比

|              数据集              | MOS分数 |
| :------------------------------: | :-----: |
|     ASVSpoof2019LA train set     |  2.939  |
|      ASVSpoof2019LA dev set      |  2.920  |
|     ASVSpoof2019LA test set      |  3.135  |
|     ASVSpoof5 train&dev set      |  2.897  |
| ASVSpoof5 test set（只含TTS/VC） |  2.071  |



### 特征选择：

- 在**CQT特征**上跑了ASVSpoof2019和ASVSpoof5（**只含TTS和VC类型**）
- **LFCC特征**在ASVSpoof2019数据集上效果不如CQT特征

### 损失函数选择：

- 使用了OHEM（困难样本挖掘）
- 使用了AAM-Softmax
- 未使用OC-Softmax

## 自监督模型

### SSL

WavLM下载地址：[WavLM预训练模型](https://github.com/microsoft/unilm/tree/master/wavlm)

| 模型规格        | Transformer 层数 | 模型维度 | 总参数量 (约)      |
| :-------------- | :--------------- | :------- | :----------------- |
| **WavLM Base**  | 12               | 768      | **~9500万 (95M)**  |
| **WavLM Base+** | 12               | 768      | **~9500万 (95M)**  |
| **WavLM Large** | 24               | 1024     | **~3.16亿 (316M)** |

### Whisper

Whisper下载地址：[OpenAI -Whisper](https://huggingface.co/openai/models)

| 模型                            | Encoder 层数 | 每层维度(d_model) | 参数量 |
| ------------------------------- | ------------ | ----------------- | ------ |
| **tiny**                        | **4 层**     | 384               | 39M    |
| **base**                        | **6 层**     | 512               | 74M    |
| **small**                       | **12 层**    | 768               | 244M   |
| **medium**                      | **24 层**    | 1024              | 769M   |
| **large / large-v2 / large-v3** | **32 层**    | 1280              | 1550M  |

### 📘 训练数据对比总表

| 模型                           | 训练数据量                     | 训练语料来源                                                 | 是否使用 MLS               | 语言数               | 训练特征                       |
| ------------------------------ | ------------------------------ | ------------------------------------------------------------ | -------------------------- | -------------------- | ------------------------------ |
| **Wav2Vec2-Base**              | **960 h**                      | LibriSpeech（未标注）                                        | ❌ **未使用**               | 1（英语）            | 干净读书音频                   |
| **Wav2Vec2-Large (LV-60K)**    | **60,000 h**                   | LibriLight 60K                                               | ❌ **未使用**               | 1（英语）            | 大规模朗读音频、相对干净       |
| **Wav2Vec2-XLSR-53**           | **56,000 h**                   | **MLS**, CommonVoice, BABEL, VoxPopuli                       | ✅ **使用 MLS**（主力数据） | 53                   | 多语言、跨域，较干净           |
| **XLS-R 300M / 1B / 2B**       | **436,000 h**                  | **MLS**, CommonVoice, BABEL, VoxLingua107、VoxPopuli、YouTube | ✅ **使用 MLS**（重要部分） | 128                  | 超大规模、多语言、噪声复杂     |
| **HuBERT-Base**                | **960 h**                      | LibriSpeech                                                  | ❌ 未使用                   | 1（英语）            | 干净朗读                       |
| **HuBERT-Large**               | **60,000 h**                   | LibriLight 60K + LibriSpeech                                 | ❌ 未使用                   | 1（英语）            | 大规模朗读音频                 |
| **HuBERT-XLSR**                | **436,000 h（与 XLS-R 相同）** | **MLS**, CommonVoice, BABEL, VoxLingua107                    | ✅ 使用 MLS                 | 128                  | 多语言、跨域                   |
| **WavLM-Base / Base+ / Large** | **94,000 h**                   | GigaSpeech、LibriLight、VGGSound、AudioSet（人声部分）       | ❌ 未使用                   | 1（英语/混合多领域） | 包含真实噪声、环境声、重叠语音 |

## SOTA

|                                                              | 模型          | 2019LA（%） | 2021LA（%） | 2021DF（%） | spoof5（%） |
| :----------------------------------------------------------: | ------------- | :---------: | :---------: | :---------: | :---------: |
| 12L-WavLM-Large AttM-LSTM（Attentive Merging of Hidden Embeddings from Pre-trained Speech Model for Anti-spoofing Detection） | SSL           |    0.65     |    3.50     |    3.19     |      -      |
| XLS-R+Mamba（Leveraging SSL Speech Features and Mamba for Enhanced DeepFake Detection） | SSL           |    0.11     |    1.78     |    1.51     |             |
| XLSR-GRKAN-Conformer + TCM（Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models） | SSL           |      -      |    0.79     |    2.54     |             |
| 融合模型（多SSL提取特征+多时间长度融合）（Temporal variability and multi-viewed self-supervised representations to tackle the ASVspoof5 Deepfake Challenge） | SSL           |      -      |      -      |      -      |    7.72     |
| MoLEx: Mixture of LoRA Experts in Speech Self-Supervised Models for Audio Deepfake Detection | SSL、LoRA微调 |             |             |             |    5.56     |



## Spoof5数据集：

### 训练集（train set）

全部都是TTS合成语音（**8种类型**、**全部使用声码器**）

![1762174580247](深度学习.assets/1762174580247.png)

### 验证集（dev set）

（**5种TTS类型**）

![1762174734812](深度学习.assets/1762174734812.png)

### 测试集（eval set）

（**6种TTS类型**）

![1762174901498](深度学习.assets/1762174901498.png)

## ASVSpoof2019数据集：

### 训练集

6种类型（**4种TTS+2种VC**）

![1762229634179](深度学习.assets/1762229634179.png)

### 验证集

6种类型（**4种TTS+2种VC**）

![1762229665877](深度学习.assets/1762229665877.png)

### 测试集

13种类型（**7种TTS+3种TTS_VC+3种VC**）

![1762229707221](深度学习.assets/1762229707221.png)

# 合成

*EchoFlow-TTS：Few-shot / Zero-shot Speaker Adaptation with Contrastive Flow Matching*

![1758372143478](深度学习.assets/1758372143478.png)

## 简化Matcha-TTS模型

- 需要修改[clcarwin/抹茶 TTS 实验 --- clcarwin/Matcha-TTS-experiment](https://github.com/clcarwin/Matcha-TTS-experiment)中的Matcha/text/cleaners.py文件（将原Matcha-TTS代码中的cleaners.py文件copy到上述文件夹下即可）（因为训练集中含有特殊符号（）、[]等）  

## 步骤

1. 先复现 basic conditional FM baseline（Matcha-TTS 的实现），确认 flow 回归稳定，再加入对比项做 ablation。Matcha-TTS  给出了 flow-matching 在 TTS/音频上的实现细节与经验。简化后的Matcha-TTS模型：[clcarwin/抹茶 TTS 实验 --- clcarwin/Matcha-TTS-experiment](https://github.com/clcarwin/Matcha-TTS-experiment)
2. 将CV方向中的代码读明白，找到哪部分是Contrastive Flow Matching模型，代码地址：[gstoica27/DeltaFM: [ICCV 2025\] Official Implementation of Contrastive Flow Matching](https://github.com/gstoica27/DeltaFM)
3. 明白Flow Matching（OT）、Rectified FLow Matching和Contrastive FLow Matching思想，推导过程
4. 将CV中的代码迁移至Matcha-TTS中，先试用小样本观察效果，然后使用大规模数据集进行训练。
5. 
6. 

## 实验对照组

1. 在这三种模型中音色的对比（Flow Matching（OT）、Rectified FLow Matching和Contrastive FLow Matching）
2. 与Glow-TTS、YourTTS 和Voiceflow合成速度的对比
3. 与

本模型 **EchoFlow**与Matcha-TTS、本模型 **EchoFlow**与Voiceflow

- **Zero-shot 输入量敏感性**：reference 长度从 1s → 60s 的影响曲线。
- **统一 speaker encoder vs 各自内置 encoder 的影响**：确保结论不是 encoder 差异驱动的。
- **与 VoiceFlow（论文实现）step-count 对齐**：比较在相同采样步数下的质量/速度权衡。[arXiv+1](https://arxiv.org/abs/2309.05027?utm_source=chatgpt.com)

## Ablation 实验

1. **CFM loss 权重消融**：不同对比损失权重下的表现。
2. **负样本策略消融**：随机负样本 / 难负样本（同性别/相近音色） / 语义相同但说话人不同。
3.  在语音领域，Matcha-TTS / SpeechFlow / FlowSE 的训练/评估 protocol 可作为参考。[SciSpace+2arXiv+2](https://scispace.com/pdf/matcha-tts-a-fast-tts-architecture-with-conditional-flow-2og66rpvlm.pdf?utm_source=chatgpt.com)

![1758347823971](深度学习.assets/1758347823971.png)

## 评估指标

- **自然度**：MOS (Mean Opinion Score)。
- **音色一致性**：speaker similarity MOS，speaker embedding cosine similarity。
- **鲁棒性**：不同样本数下的性能曲线。
- **效率**：合成速度与训练时间。

## 实验结果与分析

- CFM+Matcha-TTS 在 zero-shot 条件下 timbre consistency 明显提升。
- Few-shot 条件下，CFM 更好地利用有限数据，MOS 与 speaker similarity 高于对比模型。
- 与 Glow-TTS、YourTTS 相比，合成速度与鲁棒性有明显优势。
- 与 AdaSpeech-4、Meta-TTS 比较时，音色保持性更优，但适配效率略逊。

# xxx

1. 在Zero-shot TTS中，prompt Speech必须很清晰（如果其中含有噪声就会导致合成的语音效果很一般）
   - 想法：在模型中加入类似降噪的模块，使得在训练的时候就可以预先将含有噪声的audio进行降噪处理（让训练出来的模型隐式的学到自动去噪的能力）这样训练出来的模型鲁棒性更好。在推理时使用含噪声的Prompt Speech也不会影响合成的语音质量。
   - Facebook AI的**wav2vec 2.0**，一种自监督学习的语音表示学习方法。该方法能够有效地从噪声中提取语音特征，对噪声进行建模，从而提高语音识别的鲁棒性。（结合Maskgct中使用wav2vec2.0提SSL feature的方法）
   - 较短的prompt Speech也是可以学到东西的（我们模型只需要从prompt speech中学习语音的音色），text和prompt speech不是对应的，
   
2. 在语音增强（SE）和语音合成（TTS）中使用Flow-matching

   - SE：
     - 输入：含噪声的Mel、clean speech+高斯噪声的Mel、text condition
     - 输出：predicted Mel
   - TTS：
     - 输入：高斯噪声Mel、掩码的clean speech Mel、text condition
     - 输出：predicted Mel

   ![1761310560437](深度学习.assets/1761310560437.png)

3. 